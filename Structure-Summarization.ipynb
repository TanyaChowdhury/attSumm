{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import tqdm\n",
    "import gc\n",
    "import math\n",
    "import unicodedata\n",
    "import itertools\n",
    "from six.moves import zip_longest\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"rnn_cell\", \"lstm\", \"rnn cell\")\n",
    "flags.DEFINE_string(\"data_file\", \"Data/CQA_bin.pkl\", \"data_file\")\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 16, \"batch_size\")\n",
    "flags.DEFINE_integer(\"epochs\", 30, \"epochs\")\n",
    "\n",
    "flags.DEFINE_integer(\"dim_str\", 50, \"dim_str\")\n",
    "flags.DEFINE_integer(\"dim_sem\", 75, \"dim_sem\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.7, \"keep_prob\")\n",
    "flags.DEFINE_string(\"opt\", 'Adagrad', \"opt\")\n",
    "flags.DEFINE_float(\"lr\", 0.05, \"lr\")\n",
    "flags.DEFINE_float(\"norm\", 1e-4, \"norm\")\n",
    "flags.DEFINE_integer(\"gpu\", 0, \"gpu\")\n",
    "\n",
    "flags.DEFINE_string(\"sent_attention\", \"max\", \"sent_attention\")\n",
    "flags.DEFINE_string(\"ans_attention\", \"max\", \"ans_attention\")\n",
    "flags.DEFINE_string(\"doc_attention\", \"max\", \"doc_attention\")\n",
    "flags.DEFINE_bool(\"large_data\", True, \"large_data\")\n",
    "flags.DEFINE_integer(\"log_period\", 5000, \"log_period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None, shorten=False, num_groups=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue,) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = (tuple(e for e in each if e is not None) for each in out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self):\n",
    "        self.token_idxs = None\n",
    "        self.abstract_idxs = None\n",
    "        self.idx = -1\n",
    "\n",
    "    def _doc_len(self):\n",
    "        k = len(self.token_idxs)\n",
    "        return (k)\n",
    "\n",
    "    def _abstract_len(self):\n",
    "        k = len(self.abstract_idxs)\n",
    "        return k\n",
    "\n",
    "    def _max_ans_len(self):\n",
    "        k = max([len(ans) for ans in self.token_idxs])\n",
    "        return int(k)\n",
    "    \n",
    "    def _max_sent_len(self):\n",
    "        k = max([len(sent) for ans in self.token_idxs for sent in ans ])\n",
    "        return int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_examples = len(self.data)\n",
    "\n",
    "    def sort(self):\n",
    "        random.shuffle(self.data)\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_sent_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_ans_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._doc_len())\n",
    "\n",
    "    def get_by_idxs(self, idxs):\n",
    "        return [self.data[idx] for idx in idxs]\n",
    "\n",
    "    def get_batches(self, batch_size, num_epochs=None, rand = True):\n",
    "        num_batches_per_epoch = int(math.ceil(self.num_examples / batch_size))\n",
    "        idxs = list(range(self.num_examples))\n",
    "        _grouped = lambda: list(grouper(idxs, batch_size))\n",
    "\n",
    "        if(rand):\n",
    "            grouped = lambda: random.sample(_grouped(), num_batches_per_epoch)\n",
    "        else:\n",
    "            grouped = _grouped\n",
    "        num_steps = num_epochs*num_batches_per_epoch\n",
    "        batch_idx_tuples = itertools.chain.from_iterable(grouped() for _ in range(num_epochs))\n",
    "        for i in range(num_steps):\n",
    "            batch_idxs = tuple(i for i in next(batch_idx_tuples) if i is not None)\n",
    "            batch_data = self.get_by_idxs(batch_idxs)\n",
    "            yield i,batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LReLu(x, leak=0.01):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * tf.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicBiRNN(input, seqlen, n_hidden, cell_type, cell_name=''):\n",
    "    batch_size = tf.shape(input)[0]\n",
    "    with tf.variable_scope(cell_name + 'fw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            fw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            fw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "\n",
    "        fw_initial_state = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    with tf.variable_scope(cell_name + 'bw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            bw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "        bw_initial_state = bw_cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(cell_name):\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, input,\n",
    "                                                                 initial_state_fw=fw_initial_state,\n",
    "                                                                 initial_state_bw=bw_initial_state,\n",
    "                                                                 sequence_length=seqlen)\n",
    "    return outputs, output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(helper, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units, memory=encoder_outputs,memory_sequence_length=input_lengths)\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(attn_cell, vocab_size, reuse=reuse)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell=out_cell, helper=helper,initial_state=out_cell.zero_state(dtype=tf.float32, batch_size=batch_size))#initial_state=encoder_final_state)\n",
    "        outputs = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, output_time_major=False,impute_finished=True, maximum_iterations=output_max_length)\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure(name, input, max_l, mask_parser_1, mask_parser_2):\n",
    "    def _getDep(input, mask1, mask2):\n",
    "        #input: batch_l, sent_l, rnn_size\n",
    "        with tf.variable_scope(\"Structure/\"+name, reuse=True, dtype=tf.float32):\n",
    "            w_parser_p = tf.get_variable(\"w_parser_p\")\n",
    "            w_parser_c = tf.get_variable(\"w_parser_c\")\n",
    "            b_parser_p = tf.get_variable(\"bias_parser_p\")\n",
    "            b_parser_c = tf.get_variable(\"bias_parser_c\")\n",
    "\n",
    "            w_parser_s = tf.get_variable(\"w_parser_s\")\n",
    "            w_parser_root = tf.get_variable(\"w_parser_root\")\n",
    "\n",
    "        parent = tf.tanh(tf.tensordot(input, w_parser_p, [[2], [0]]) + b_parser_p)\n",
    "        child = tf.tanh(tf.tensordot(input, w_parser_c, [[2], [0]])+b_parser_c)\n",
    "        # rep = LReLu(parent+child)\n",
    "        temp = tf.tensordot(parent,w_parser_s,[[-1],[0]])\n",
    "        raw_scores_words_ = tf.matmul(temp,tf.matrix_transpose(child))\n",
    "\n",
    "        # raw_scores_words_ = tf.squeeze(tf.tensordot(rep, w_parser_s, [[3], [0]]) , [3])\n",
    "        raw_scores_root_ = tf.squeeze(tf.tensordot(input, w_parser_root, [[2], [0]]) , [2])\n",
    "        raw_scores_words = tf.exp(raw_scores_words_)\n",
    "        raw_scores_root = tf.exp(raw_scores_root_)\n",
    "        tmp = tf.zeros_like(raw_scores_words[:,:,0])\n",
    "        raw_scores_words = tf.matrix_set_diag(raw_scores_words,tmp)\n",
    "\n",
    "        str_scores, LL = _getMatrixTree(raw_scores_root, raw_scores_words, mask1, mask2)\n",
    "        return str_scores\n",
    "\n",
    "    def _getMatrixTree(r, A, mask1, mask2):\n",
    "        L = tf.reduce_sum(A, 1)\n",
    "        L = tf.matrix_diag(L)\n",
    "        L = L - A\n",
    "\n",
    "        LL = L[:, 1:, :]\n",
    "        LL = tf.concat([tf.expand_dims(r, [1]), LL], 1)\n",
    "        LL_inv = tf.matrix_inverse(LL)  #batch_l, doc_l, doc_l\n",
    "        d0 = tf.multiply(r, LL_inv[:, :, 0])\n",
    "        LL_inv_diag = tf.expand_dims(tf.matrix_diag_part(LL_inv), 2)\n",
    "        tmp1 = tf.matrix_transpose(tf.multiply(tf.matrix_transpose(A), LL_inv_diag))\n",
    "        tmp2 = tf.multiply(A, tf.matrix_transpose(LL_inv))\n",
    "        d = mask1 * tmp1 - mask2 * tmp2\n",
    "        d = tf.concat([tf.expand_dims(d0,[1]), d], 1)\n",
    "        return d, LL\n",
    "\n",
    "    str_scores = _getDep(input, mask_parser_1, mask_parser_2)\n",
    "    return str_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureModel():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        t_variables = {}\n",
    "        t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        t_variables['batch_l'] = tf.placeholder(tf.int32)\n",
    "        \n",
    "        #Placeholder for answers and abstracts\n",
    "        t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None, None])\n",
    "        t_variables['abstract_idxs'] = tf.placeholder(tf.int32, [None,None])\n",
    "        t_variables['generated_idxs'] = tf.placeholder(tf.int32,[None,None])\n",
    "\n",
    "        #Storing length of each heirarchy element\n",
    "        t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None,None])\n",
    "        t_variables['ans_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "        t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "        t_variables['abstract_l'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        #Storing upper limit of each element length\n",
    "        t_variables['max_sent_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_doc_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_ans_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_abstract_l'] = tf.placeholder(tf.int32)\n",
    "\n",
    "        #Masks to limit element sizes\n",
    "        t_variables['mask_tokens'] = tf.placeholder(tf.float32, [None, None, None,None])\n",
    "        t_variables['mask_sents'] = tf.placeholder(tf.float32, [None, None,None])\n",
    "        t_variables['mask_answers']= tf.placeholder(tf.float32,[None,None])\n",
    "        \n",
    "        #Parser Masks\n",
    "        t_variables['mask_parser_1'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "        t_variables['mask_parser_2'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "\n",
    "        t_variables['start_tokens'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        \n",
    "        self.t_variables = t_variables\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        abstracts_l_matrix = np.zeros([batch_size],np.int32)\n",
    "        doc_l_matrix = np.zeros([batch_size], np.int32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_ans = len(instance.token_idxs)\n",
    "            n_words = len(instance.abstract_idxs)\n",
    "            doc_l_matrix[i] = n_ans\n",
    "            abstracts_l_matrix[i] = n_words\n",
    "        \n",
    "        max_doc_l = np.max(doc_l_matrix)\n",
    "        max_ans_l = max([max([len(ans) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_sent_l = max([max([max([len(sent) for itr in doc.token_idxs for sent in itr]) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_abstract_l = np.max(abstracts_l_matrix)\n",
    "\n",
    "        ans_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "        sent_l_matrix = np.zeros([batch_size, max_doc_l, max_ans_l], np.int32)\n",
    "\n",
    "        token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_ans_l, max_sent_l], np.int32)\n",
    "        abstract_idx_matrix = np.zeros([batch_size,max_abstract_l], np.int32)\n",
    "\n",
    "        mask_tokens_matrix = np.ones([batch_size, max_doc_l, max_ans_l, max_sent_l], np.float32)\n",
    "        mask_sents_matrix = np.ones([batch_size, max_doc_l, max_ans_l], np.float32)\n",
    "        mask_answers_matrix = np.ones([batch_size, max_doc_l],np.float32)\n",
    "        mask_abstact_matrix = np.ones([batch_size,max_abstract_l],np.float32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_answers = len(instance.token_idxs)\n",
    "            abstract_ = instance.abstract_idxs\n",
    "            abstract_idx_matrix[i,:len(abstract_)] = np.asarray(abstract_)\n",
    "            mask_abstact_matrix[i,len(abstract_):] = 0\n",
    "            abstracts_l_matrix[i] = len(abstract_)\n",
    "\n",
    "            for j, ans in enumerate(instance.token_idxs):\n",
    "                for k, sent in enumerate(instance.token_idxs[j]):\n",
    "                    token_idxs_matrix[i, j, k,:len(sent)] = np.asarray(sent)\n",
    "                    mask_tokens_matrix[i, j, k,len(sent):] = 0\n",
    "                    sent_l_matrix[i, j,k] = len(sent)\n",
    "\n",
    "                mask_sents_matrix[i,j,len(ans):]=0\n",
    "                ans_l_matrix[i,j] = len(ans)\n",
    "\n",
    "            mask_answers_matrix[i, n_answers:] = 0\n",
    "        \n",
    "        mask_parser_1 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_2 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_1[:, :, 0] = 0\n",
    "        mask_parser_2[:, 0, :] = 0\n",
    "        \n",
    "        feed_dict = {self.t_variables['token_idxs']: token_idxs_matrix,self.t_variables['abstract_idxs']: abstract_idx_matrix,\n",
    "                     self.t_variables['sent_l']: sent_l_matrix,self.t_variables['ans_l']:ans_l_matrix,self.t_variables['doc_l']: doc_l_matrix, \n",
    "                     self.t_variables['mask_tokens']: mask_tokens_matrix, self.t_variables['mask_sents']: mask_sents_matrix, self.t_variables['mask_answers']:mask_answers_matrix,\n",
    "                     self.t_variables['abstract_l']:abstracts_l_matrix,\n",
    "                     self.t_variables['max_sent_l']: max_sent_l,self.t_variables['max_ans_l']:max_ans_l, self.t_variables['max_doc_l']: max_doc_l,self.t_variables['max_abstract_l']: max_abstract_l,\n",
    "                     self.t_variables['mask_parser_1']: mask_parser_1, self.t_variables['mask_parser_2']: mask_parser_2,\n",
    "                     self.t_variables['batch_l']: batch_size, self.t_variables['keep_prob']:self.config.keep_prob}\n",
    "        \n",
    "        return  feed_dict\n",
    "\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(\"Embeddings\"):\n",
    "            #Initial embedding placeholders\n",
    "            self.embeddings = tf.get_variable(\"emb\", [self.config.n_embed, self.config.d_embed], dtype=tf.float32,\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root = tf.get_variable(\"emb_root\", [1, 1, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_a = tf.get_variable(\"emb_root_ans\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_s = tf.get_variable(\"emb_root_s\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Model\"):\n",
    "            #Weights and biases at pooling layers and final softmax for output. (Fianl might not be required)(Semantic combination part)\n",
    "            w_comb = tf.get_variable(\"w_comb\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb = tf.get_variable(\"bias_comb\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_a = tf.get_variable(\"w_comb_a\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_a = tf.get_variable(\"bias_comb_a\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_s = tf.get_variable(\"w_comb_s\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_s = tf.get_variable(\"bias_comb_s\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/doc\"):\n",
    "            #Placeholders for hierarchical model at document level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/ans\"):\n",
    "            #Placeholders for  hierarchial model at answer level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/sent\"):\n",
    "            #Placeholders for hierarchial model at sentence level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        #Variables of dimension batchsize passing length of each vector to architectures\n",
    "        sent_l = self.t_variables['sent_l']\n",
    "        ans_l = self.t_variables['ans_l']\n",
    "        doc_l = self.t_variables['doc_l']\n",
    "        abstract_l = self.t_variables['abstract_l']\n",
    "        \n",
    "        #Maximum lengths of sentences, answers and documents to be processed\n",
    "        max_sent_l = self.t_variables['max_sent_l']\n",
    "        max_ans_l = self.t_variables['max_ans_l']\n",
    "        max_doc_l = self.t_variables['max_doc_l']\n",
    "        max_abstract_l = self.t_variables['max_abstract_l']\n",
    "\n",
    "        #batch size\n",
    "        batch_l = self.t_variables['batch_l']\n",
    "\n",
    "        #Creating embedding matrices for answers and abstracts corresponding to indexes\n",
    "        tokens_input = tf.nn.embedding_lookup(self.embeddings, self.t_variables['token_idxs'][:,:max_doc_l, :max_ans_l, :max_sent_l])\n",
    "        reference_input = tf.nn.embedding_lookup(self.embeddings,self.t_variables['abstract_idxs'][:,:max_abstract_l])\n",
    "        \n",
    "        #Dropout on input\n",
    "        tokens_input = tf.nn.dropout(tokens_input, self.t_variables['keep_prob'])\n",
    "\n",
    "        #Masking inputs\n",
    "        mask_tokens = self.t_variables['mask_tokens'][:,:max_doc_l, :max_ans_l, :max_sent_l]\n",
    "        mask_sents = self.t_variables['mask_sents'][:, :max_doc_l,:max_ans_l]\n",
    "        mask_answers = self.t_variables['mask_answers'][:,:max_doc_l]\n",
    "\n",
    "\n",
    "        [_, _, _, _, rnn_size] = tokens_input.get_shape().as_list()\n",
    "        tokens_input_do = tf.reshape(tokens_input, [batch_l * max_doc_l*max_ans_l, max_sent_l, rnn_size])\n",
    "\n",
    "        sent_l = tf.reshape(sent_l, [batch_l * max_doc_l* max_ans_l])\n",
    "        mask_tokens = tf.reshape(mask_tokens, [batch_l * max_doc_l*max_ans_l, -1])\n",
    "\n",
    "        #Word level input\n",
    "        tokens_output, _ = dynamicBiRNN(tokens_input_do, sent_l, n_hidden=self.config.dim_hidden,\n",
    "                                        cell_type=self.config.rnn_cell, cell_name='Model/sent')\n",
    "        \n",
    "        tokens_sem = tf.concat([tokens_output[0][:,:,:self.config.dim_sem], tokens_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        tokens_str = tf.concat([tokens_output[0][:,:,self.config.dim_sem:], tokens_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "        \n",
    "        temp1 = tf.zeros([batch_l * max_doc_l*max_ans_l, max_sent_l,1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l*max_ans_l ,1,max_sent_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l, max_sent_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l-1, max_sent_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_s_ = get_structure('sent', tokens_str, max_sent_l, mask1, mask2)  # batch_l,  sent_l+1, sent_l\n",
    "        str_scores_s = tf.matrix_transpose(str_scores_s_)  # soft parent\n",
    "        tokens_sem_root = tf.concat([tf.tile(embeddings_root_s, [batch_l * max_doc_l *max_ans_l, 1, 1]), tokens_sem], 1)\n",
    "        tokens_output_ = tf.matmul(str_scores_s, tokens_sem_root)\n",
    "        tokens_output = LReLu(tf.tensordot(tf.concat([tokens_sem, tokens_output_], 2), w_comb_s, [[2], [0]]) + b_comb_s)\n",
    "\n",
    "        if (self.config.sent_attention == 'sum'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)\n",
    "        elif (self.config.sent_attention == 'mean'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)/tf.expand_dims(tf.cast(sent_l,tf.float32),1)\n",
    "        elif (self.config.sent_attention == 'max'):\n",
    "            tokens_output = tokens_output + tf.expand_dims((mask_tokens-1)*999,2)\n",
    "            tokens_output = tf.reduce_max(tokens_output, 1)\n",
    "\n",
    "        #Sentence level RNN\n",
    "        sents_input = tf.reshape(tokens_output, [batch_l*max_doc_l, max_ans_l,2*self.config.dim_sem])\n",
    "        ans_l = tf.reshape(ans_l,[batch_l*max_doc_l])\n",
    "        mask_sents = tf.reshape(mask_sents,[batch_l*max_doc_l,-1])\n",
    "\n",
    "        sents_output, _ = dynamicBiRNN(sents_input, ans_l, n_hidden=self.config.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/ans')\n",
    "\n",
    "        sents_sem = tf.concat([sents_output[0][:,:,:self.config.dim_sem], sents_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        sents_str = tf.concat([sents_output[0][:,:,self.config.dim_sem:], sents_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        temp1 = tf.zeros([batch_l * max_doc_l, max_ans_l, 1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l, 1, max_ans_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l , max_ans_l, max_ans_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l , max_ans_l-1, max_ans_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_ = get_structure('ans', sents_str, max_ans_l, mask1,mask2)  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        sents_sem_root = tf.concat([tf.tile(embeddings_root_a, [batch_l*max_doc_l, 1, 1]), sents_sem], 1)\n",
    "        sents_output_ = tf.matmul(str_scores, sents_sem_root)\n",
    "        sents_output = LReLu(tf.tensordot(tf.concat([sents_sem, sents_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.doc_attention == 'sum'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)\n",
    "        elif (self.config.doc_attention == 'mean'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)/tf.expand_dims(tf.cast(ans_l,tf.float32),1)\n",
    "        elif (self.config.doc_attention == 'max'):\n",
    "            sents_output = sents_output + tf.expand_dims((mask_sents-1)*999,2)\n",
    "            sents_output = tf.reduce_max(sents_output, 1)\n",
    "\n",
    "        #Answer level RNN\n",
    "        ans_input = tf.reshape(sents_output, [batch_l, max_doc_l,2*self.config.dim_sem])\n",
    "        ans_output, _ = dynamicBiRNN(ans_input, doc_l, n_hidden=self.config.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/doc')\n",
    "\n",
    "        ans_sem = tf.concat([ans_output[0][:,:,:self.config.dim_sem], ans_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        ans_str = tf.concat([ans_output[0][:,:,self.config.dim_sem:], ans_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        str_scores_ = get_structure('doc', ans_str, max_doc_l, self.t_variables['mask_parser_1'], self.t_variables['mask_parser_2'])  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        ans_sem_root = tf.concat([tf.tile(embeddings_root, [batch_l, 1, 1]), ans_sem], 1)\n",
    "        ans_output_ = tf.matmul(str_scores, ans_sem_root)\n",
    "        ans_output = LReLu(tf.tensordot(tf.concat([ans_sem, ans_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.ans_attention == 'sum'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            # ans_output = tf.reduce_sum(ans_output, 1)\n",
    "        elif (self.config.ans_attention == 'mean'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            ans_output = tf.reduce_sum(ans_output, 1)/tf.expand_dims(tf.cast(doc_l,tf.float32),1)\n",
    "        elif (self.config.ans_attention == 'max'):\n",
    "            ans_output = ans_output + tf.expand_dims((mask_answers-1)*999,2)\n",
    "            ans_output = tf.reduce_max(ans_output, 1)\n",
    "\n",
    "        print(\"Encoder pooled ans_output shape\", ans_output.shape)\n",
    "        \n",
    "        tgt_vocab_size = self.config.vsize\n",
    "        learning_rate = self.config.lr\n",
    "        \n",
    "        #Is the dimension correct?\n",
    "        decoder_input = tf.reshape(ans_output,[batch_l,2*self.config.dim_sem])\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(2*self.config.dim_sem)\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(reference_input, abstract_l, time_major=True)\n",
    "        projection_layer = tf.layers.Dense(tgt_vocab_size, use_bias=False)\n",
    "        \n",
    "        #How to initialize the LSTM?\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,tf.contrib.rnn.LSTMStateTuple(decoder_input,decoder_input),output_layer=projection_layer)\n",
    "        \n",
    "        outputs, states, seq_l = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        print(\"rnn_output.shape=\", outputs.rnn_output.shape)\n",
    "        print(\"sample_id.shape=\", outputs.sample_id.shape)\n",
    "        print(\"final_state=\", states)\n",
    "        print(\"final_sequence_lengths.shape=\", seq_l.shape)\n",
    "\n",
    "        #Returns loss. Generated index is initialized [none,none]. Should be [batch_size,decoder length]\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.t_variables['generated_idxs'], logits=logits)\n",
    "\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        update_step = optimizer.apply_gradients(zip(clipped_gradients, params),global_step=global_step)\n",
    "\n",
    "        self.final_output = logits\n",
    "        self.loss = loss\n",
    "        self.opt = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, model, test_batches):\n",
    "    corr_count, all_count = 0, 0\n",
    "    for ct, batch in test_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        feed_dict[model.t_variables['keep_prob']] = 1\n",
    "        predictions = sess.run(model.final_output, feed_dict=feed_dict)\n",
    "\n",
    "        predictions = np.argmax(predictions, 1)\n",
    "        corr_count += np.sum(predictions == feed_dict[model.t_variables['gold_labels']])\n",
    "        all_count += len(batch)\n",
    "    acc_test = 1.0 * corr_count / all_count\n",
    "    return  acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = flags.FLAGS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu)\n",
    "\n",
    "import random\n",
    "\n",
    "hash = random.getrandbits(32)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ah = logging.FileHandler(str(hash)+'.log')\n",
    "ah.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "ah.setFormatter(formatter)\n",
    "logger.addHandler(ah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fa9867e35469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data loaded succesfully'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unsupported pickle protocol: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPROTO\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 3"
     ]
    }
   ],
   "source": [
    "gc.disable()\n",
    "train, dev, test, embeddings, vocab = pickle.load(open(config.data_file,'rb'))\n",
    "gc.enable()\n",
    "print('Data loaded succesfully')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, devset, testset = DataSet(train), DataSet(dev), DataSet(test)\n",
    "vocab = dict([(v.index,k) for k,v in vocab.items()])\n",
    "trainset.sort()\n",
    "train_batches = trainset.get_batches(config.batch_size, config.epochs, rand=True)\n",
    "dev_batches = devset.get_batches(config.batch_size, 1, rand=False)\n",
    "test_batches = testset.get_batches(config.batch_size, 1, rand=False)\n",
    "dev_batches = [i for i in dev_batches]\n",
    "test_batches = [i for i in test_batches]\n",
    "\n",
    "num_examples, train_batches, dev_batches, test_batches, embedding_matrix, vocab = len(train), train_batches, dev_batches, test_batches, embeddings, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_embed, config.d_embed = embedding_matrix.shape\n",
    "config.vocab = vocab\n",
    "config.vsize = len(vocab)\n",
    "config.inv_vocab = {v: k for k, v in vocab.items()}\n",
    "config.dim_hidden = config.dim_sem+config.dim_str\n",
    "\n",
    "# print(config.__flags)\n",
    "logger.critical(str(config.__flags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StructureModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/114120 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tried to read from index 27 but array size is: 16\n\t [[Node: decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_1, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_2)]]\n\nCaused by op 'decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-2f52417d930d>\", line 1, in <module>\n    model.build()\n  File \"<ipython-input-10-e864fd2c9335>\", line 319, in build\n    outputs, states,seq_l = tf.contrib.seq2seq.dynamic_decode(decoder)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 147, in step\n    sample_ids=sample_ids)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 241, in next_inputs\n    lambda: nest.map_structure(read_from_ta, self._input_tas))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\n    original_result = fn()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 241, in <lambda>\n    lambda: nest.map_structure(read_from_ta, self._input_tas))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 413, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 413, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 238, in read_from_ta\n    return inp.read(next_time)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 49, in fn\n    return method(self, *args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 49, in fn\n    return method(self, *args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 281, in read\n    name=name)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4445, in _tensor_array_read_v3\n    dtype=dtype, name=name)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Tried to read from index 27 but array size is: 16\n\t [[Node: decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_1, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tried to read from index 27 but array size is: 16\n\t [[Node: decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_1, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-56d17848cb7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_period\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tried to read from index 27 but array size is: 16\n\t [[Node: decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_1, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_2)]]\n\nCaused by op 'decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-2f52417d930d>\", line 1, in <module>\n    model.build()\n  File \"<ipython-input-10-e864fd2c9335>\", line 319, in build\n    outputs, states,seq_l = tf.contrib.seq2seq.dynamic_decode(decoder)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 147, in step\n    sample_ids=sample_ids)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 241, in next_inputs\n    lambda: nest.map_structure(read_from_ta, self._input_tas))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\n    original_result = fn()\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 241, in <lambda>\n    lambda: nest.map_structure(read_from_ta, self._input_tas))\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 413, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 413, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/helper.py\", line 238, in read_from_ta\n    return inp.read(next_time)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 49, in fn\n    return method(self, *args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 49, in fn\n    return method(self, *args, **kwargs)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 281, in read\n    name=name)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4445, in _tensor_array_read_v3\n    dtype=dtype, name=name)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/tanya14109/myenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Tried to read from index 27 but array size is: 16\n\t [[Node: decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3 = TensorArrayReadV3[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_1, decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3/Switch_2)]]\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch = int(num_examples / config.batch_size)\n",
    "num_steps = config.epochs * num_batches_per_epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    gvi = tf.global_variables_initializer()\n",
    "    sess.run(gvi)\n",
    "    sess.run(model.embeddings.assign(embedding_matrix.astype(np.float32)))\n",
    "    loss = 0\n",
    "\n",
    "    for ct, batch in tqdm.tqdm(train_batches, total=num_steps):\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        outputs,_,_loss = sess.run([model.final_output, model.opt, model.loss], feed_dict=feed_dict)\n",
    "        loss+=_loss\n",
    "        if(ct%config.log_period==0):\n",
    "            acc_test = evaluate(sess, model, test_batches)\n",
    "            acc_dev = evaluate(sess, model, dev_batches)\n",
    "            print('Step: {} Loss: {}\\n'.format(ct, loss))\n",
    "            print('Test ACC: {}\\n'.format(acc_test))\n",
    "            print('Dev  ACC: {}\\n'.format(acc_dev))\n",
    "            logger.debug('Step: {} Loss: {}\\n'.format(ct, loss))\n",
    "            logger.debug('Test ACC: {}\\n'.format(acc_test))\n",
    "            logger.debug('Dev  ACC: {}\\n'.format(acc_dev))\n",
    "            logger.handlers[0].flush()\n",
    "            loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
