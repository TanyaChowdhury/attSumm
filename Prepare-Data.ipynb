{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import unicodedata\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', unicode(s,'utf-8'))\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.doclst = {}\n",
    "\n",
    "    def load(self, in_path, name):\n",
    "        self.doclst[name] = []        \n",
    "        binaryData = []\n",
    "\n",
    "        for item in open(in_path,'r',encoding=\"utf-8\"):\n",
    "            binaryData.append(item)\n",
    "        \n",
    "        i = 0\n",
    "        while(i< len(binaryData)):\n",
    "            doc = RawData(name,int(i/3))\n",
    "\n",
    "            #Check if everything all right\n",
    "            split1 = binaryData[i]\n",
    "            if ( i!=0 and '<split1>' not in split1):\n",
    "                print('Some error in preprocess', i,\"\\n\")\n",
    "                print(split1)\n",
    "\n",
    "            #Create Answer List\n",
    "            answersString = binaryData[i+1]\n",
    "            answersString = answersString.replace('<0>','')\n",
    "            answersString = answersString.replace('\\n','')\n",
    "            answersList = answersString.split('<split2>')\n",
    "            answersList = [item.split('<split3>') for item in answersList]\n",
    "            doc.answers = answersList\n",
    "\n",
    "            #Create Abstract List\n",
    "            abstractString = binaryData[i+2]\n",
    "            abstractString = abstractString.replace('<1>','')\n",
    "            abstractString = abstractString.replace('\\n','')\n",
    "            abstractList = abstractString.split('<reference_split>')\n",
    "            abstractString = ' '.join(abstractList)\n",
    "            doc.abstract = abstractList\n",
    "            doc.abstractString = abstractString\n",
    "\n",
    "            i+=3\n",
    "\n",
    "            self.doclst[name].append(doc)\n",
    "\n",
    "    def preprocess(self):\n",
    "        for dataset in self.doclst:\n",
    "            for doc in self.doclst[dataset]:\n",
    "\n",
    "                #Cleaning and tokenizing document \n",
    "                doc.answers_sent_list = []\n",
    "                doc.document_tokens_list = []\n",
    "                for answers in doc.answers:\n",
    "                    preprocessed_sentences = []\n",
    "                    token_sentences = []\n",
    "                    for sentences in answers:\n",
    "                        s = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`_]\", \" \",sentences)\n",
    "                        preprocessed_sentences.append(s)\n",
    "                        sentence_tokens = s.split()\n",
    "                        if(len(sentence_tokens)>1):\n",
    "                            token_sentences.append(s.split())\n",
    "                    doc.answers_sent_list.append(preprocessed_sentences)\n",
    "                    doc.document_tokens_list.append(token_sentences)\n",
    "\n",
    "                #Cleaning and tokenizing abstract\n",
    "                s = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`_]\", \" \",doc.abstractString)\n",
    "                abstract_tokens = s.split()\n",
    "                doc.abstract_tokens_list = abstract_tokens\n",
    "                            \n",
    "            #Only add threads with more than 0 answers and more than 0 words abstract\n",
    "            self.doclst[dataset] = [doc for doc in self.doclst[dataset] if (len(doc.document_tokens_list)!=0 and len(doc.abstract_tokens_list)!=0)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def w2v(self, options):\n",
    "        sentences = []\n",
    "        for doc in self.doclst['train']:\n",
    "            for answers in doc.document_tokens_list:\n",
    "                for sents in answers:\n",
    "                    sentences.append(sents)\n",
    "            sentences.append(doc.abstract_tokens_list)\n",
    "        \n",
    "        if('dev' in self.doclst):\n",
    "            for doc in self.doclst['dev']:\n",
    "                for answers in doc.document_tokens_list:\n",
    "                    for sents in answers:\n",
    "                        sentences.append(sents)\n",
    "            \n",
    "                sentences.append(doc.abstract_tokens_list)\n",
    "       \n",
    "    \n",
    "        if(options['skip_gram']):\n",
    "            self.w2v_model = gensim.models.word2vec.Word2Vec(size=options['emb_size'], window=5, min_count=15, workers=4, sg=1)\n",
    "        else:\n",
    "            self.w2v_model = gensim.models.word2vec.Word2Vec(size=options['emb_size'], window=5,min_count=15, workers=4)\n",
    "        \n",
    "        self.w2v_model.scan_vocab(sentences)  # initial survey\n",
    "        rtn = self.w2v_model.scale_vocab(dry_run = True)  # trim by min_count & precalculate downsampling\n",
    "        print(rtn)\n",
    "        self.w2v_model.finalize_vocab()  # build tables & arrays\n",
    "        self.w2v_model.train(sentences, total_examples=self.w2v_model.corpus_count, epochs=self.w2v_model.iter)\n",
    "        \n",
    "        \n",
    "        self.vocab = self.w2v_model.wv.vocab\n",
    "        print('Vocab size: {}'.format(len(self.vocab)))\n",
    "    \n",
    "    def prepare(self, options):\n",
    "        instances, instances_dev, instances_test = [],[],[]\n",
    "        instances, embeddings, vocab = self.prepareData(options,'train')\n",
    "        \n",
    "        if ('dev' in self.doclst):\n",
    "            instances_dev = self.prepareData(options, 'dev')\n",
    "        \n",
    "        instances_test = self.prepareData(options, 'test')\n",
    "        return instances, instances_dev, instances_test, embeddings, vocab\n",
    "\n",
    "    def prepareData(self, options,mode):\n",
    "        instancelst = []\n",
    "\n",
    "        if(mode=='train'):        \n",
    "            #(50000,200) every word in vocab is assigned an embedding which is pre trained\n",
    "            embeddings = np.zeros([len(self.vocab)+1,options['emb_size']])\n",
    "            for word in self.vocab:\n",
    "                embeddings[self.vocab[word].index] = self.w2v_model[word]\n",
    "            \n",
    "            self.vocab['UNK'] = gensim.models.word2vec.Vocab(count=0, index=len(self.vocab))\n",
    "        \n",
    "\n",
    "        n_filtered = 0\n",
    "        \n",
    "        for i_doc, doc in enumerate(self.doclst[mode]):\n",
    "            instance = Instance()\n",
    "            instance.idx = i_doc\n",
    "\n",
    "            n_answers = len(doc.document_tokens_list)\n",
    "            max_n_sents = max([len(answer) for answer in doc.document_tokens_list])\n",
    "            max_n_tokens = max([len(sent) for answer in doc.document_tokens_list for sent in answer])\n",
    "\n",
    "            if(n_answers > options['max_answers']):\n",
    "                n_filtered+=1\n",
    "                continue\n",
    "\n",
    "            if(max_n_sents>options['max_sents']):\n",
    "                n_filtered += 1\n",
    "                continue\n",
    "            \n",
    "            if(max_n_tokens>options['max_tokens']):\n",
    "                n_filtered += 1\n",
    "                continue\n",
    "\n",
    "            #Generating document token indexes array and storing them in token_idxs of instance\n",
    "            document_token_indexes = []\n",
    "            for answer in doc.document_tokens_list:\n",
    "                sentence_indexes = []\n",
    "                for sentence in answer:\n",
    "                    token_indexes = []\n",
    "                    for token in sentence:\n",
    "                        if(token in self.vocab):\n",
    "                            token_indexes.append(self.vocab[token].index)\n",
    "                        else:\n",
    "                            token_indexes.append(self.vocab['UNK'].index)\n",
    "                    sentence_indexes.append(token_indexes)\n",
    "                document_token_indexes.append(sentence_indexes)\n",
    "            instance.token_idxs = document_token_indexes\n",
    "\n",
    "\n",
    "            #Generating abstract token indexes array and storing them in abstract_idxs of instance\n",
    "            abstract_token_indexes = []\n",
    "            for word in doc.abstract_tokens_list:\n",
    "                if(word in self.vocab):\n",
    "                    abstract_token_indexes.append(self.vocab[word].index)\n",
    "                else:\n",
    "                    abstract_token_indexes.append(self.vocab['UNK'].index)\n",
    "            instance.abstract_idxs = abstract_token_indexes\n",
    "            instancelst.append(instance)\n",
    "\n",
    "        print('n_filtered in train: {}'.format(n_filtered))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return instancelst, embeddings, self.vocab\n",
    "        else:\n",
    "            return instancelst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self):\n",
    "        self.token_idxs = None\n",
    "        self.abstract_idxs = None\n",
    "        self.idx = -1\n",
    "\n",
    "    def _doc_len(self, idx):\n",
    "        k = len(self.token_idxs)\n",
    "        return k\n",
    "\n",
    "    def _abstract_len(self,idx):\n",
    "        k = len(self.abstract_idxs)\n",
    "        return k\n",
    "\n",
    "    def _max_sent_len(self, idxs):\n",
    "        k = max([len(sent) for sent in self.token_idxs])\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_examples = len(self.data)\n",
    "\n",
    "    def sort(self):\n",
    "        random.shuffle(self.data)\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_sent_len)\n",
    "        self.data = sorted(self.data, key=lambda x: x._doc_len)\n",
    "\n",
    "    def get_by_idxs(self, idxs):\n",
    "        return [self.data[idx] for idx in idxs]\n",
    "\n",
    "    def get_batches(self, batch_size, num_epochs=None, rand = True):\n",
    "        num_batches_per_epoch = int(math.ceil(self.num_examples / batch_size))\n",
    "        idxs = list(range(self.num_examples))\n",
    "        _grouped = lambda: list(grouper(idxs, batch_size))\n",
    "\n",
    "        if(rand):\n",
    "            grouped = lambda: random.sample(_grouped(), num_batches_per_epoch)\n",
    "        else:\n",
    "            grouped = _grouped\n",
    "        num_steps = num_epochs*num_batches_per_epoch\n",
    "        batch_idx_tuples = itertools.chain.from_iterable(grouped() for _ in range(num_epochs))\n",
    "        for i in range(num_steps):\n",
    "            batch_idxs = tuple(i for i in next(batch_idx_tuples) if i is not None)\n",
    "            batch_data = self.get_by_idxs(batch_idxs)\n",
    "            yield i,batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawData:\n",
    "    def __init__(self,name,count):\n",
    "        self.idx = name+ '/'+str(count)\n",
    "        self.abstract = []\n",
    "        self.answers = []\n",
    "        self.prediction = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load('Data/CQA/cqa.train', 'train')\n",
    "corpus.load('Data/CQA/cqa.dev', 'dev')\n",
    "corpus.load('Data/CQA/cqa.test', 'test')\n",
    "corpus.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training word embeddings\n",
      "{'drop_unique': 1055240, 'retain_total': 66614711, 'downsample_unique': 50, 'downsample_total': 53812154, 'memory': {'vocab': 51776000, 'syn0': 82841600, 'syn1neg': 82841600, 'total': 217459200}}\n",
      "Vocab size: 103552\n"
     ]
    }
   ],
   "source": [
    "options =  dict(max_answers=20, max_sents=100, max_tokens=120, skip_gram=False, emb_size=200)\n",
    "print('Start training word embeddings')\n",
    "corpus.w2v(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanya14109/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:122: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_filtered in train: 10739\n",
      "n_filtered in train: 1681\n",
      "n_filtered in train: 1721\n"
     ]
    }
   ],
   "source": [
    "instance, instance_dev, instance_test, embeddings, vocab = corpus.prepare(options)\n",
    "pickle.dump((instance, instance_dev, instance_test, embeddings, vocab),open('Data/CQA_bin_15.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
