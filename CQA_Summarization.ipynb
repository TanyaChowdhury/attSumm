{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/300041707/Personal/myenv/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import tqdm\n",
    "import gc\n",
    "import math\n",
    "import unicodedata\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "from six.moves import zip_longest\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string(\"rnn_cell\", \"lstm\", \"rnn cell\")\n",
    "flags.DEFINE_string(\"data_file\", \"Data/CQA_bin.pkl\", \"data_file\")\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 16, \"batch_size\")\n",
    "flags.DEFINE_integer(\"epochs\", 30, \"epochs\")\n",
    "\n",
    "flags.DEFINE_integer(\"decoder_length\",100,\"decoder_length\")\n",
    "\n",
    "flags.DEFINE_integer(\"dim_str\", 50, \"dim_str\")\n",
    "flags.DEFINE_integer(\"dim_sem\", 75, \"dim_sem\")\n",
    "flags.DEFINE_integer(\"dim_output\", 150, \"dim_output\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.7, \"keep_prob\")\n",
    "flags.DEFINE_string(\"opt\", 'Adagrad', \"opt\")\n",
    "flags.DEFINE_float(\"lr\", 0.05, \"lr\")\n",
    "flags.DEFINE_float(\"norm\", 1e-4, \"norm\")\n",
    "flags.DEFINE_integer(\"gpu\", 0, \"gpu\")\n",
    "\n",
    "flags.DEFINE_string(\"sent_attention\", \"max\", \"sent_attention\")\n",
    "flags.DEFINE_string(\"ans_attention\", \"max\", \"ans_attention\")\n",
    "flags.DEFINE_string(\"doc_attention\", \"max\", \"doc_attention\")\n",
    "flags.DEFINE_bool(\"large_data\", True, \"large_data\")\n",
    "flags.DEFINE_integer(\"log_period\", 100, \"log_period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None, shorten=False, num_groups=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue,) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = (tuple(e for e in each if e is not None) for each in out)\n",
    "    return out\n",
    "\n",
    "def LReLu(x, leak=0.01):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * tf.abs(x)\n",
    "\n",
    "def dynamicBiRNN(input, seqlen, n_hidden, cell_type, cell_name=''):\n",
    "    batch_size = tf.shape(input)[0]\n",
    "    with tf.variable_scope(cell_name + 'fw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            fw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            fw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "\n",
    "        fw_initial_state = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    with tf.variable_scope(cell_name + 'bw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            bw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "        bw_initial_state = bw_cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(cell_name):\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, input,\n",
    "                                                                 initial_state_fw=fw_initial_state,\n",
    "                                                                 initial_state_bw=bw_initial_state,\n",
    "                                                                 sequence_length=seqlen)\n",
    "    return outputs, output_states\n",
    "\n",
    "def decode(helper, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units, memory=encoder_outputs,memory_sequence_length=input_lengths)\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(attn_cell, vocab_size, reuse=reuse)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell=out_cell, helper=helper,initial_state=out_cell.zero_state(dtype=tf.float32, batch_size=batch_size))#initial_state=encoder_final_state)\n",
    "        outputs = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, output_time_major=False,impute_finished=True, maximum_iterations=output_max_length)\n",
    "        return outputs[0]\n",
    "    \n",
    "def get_structure(name, input, max_l, mask_parser_1, mask_parser_2):\n",
    "    def _getDep(input, mask1, mask2):\n",
    "        #input: batch_l, sent_l, rnn_size\n",
    "        with tf.variable_scope(\"Structure/\"+name, reuse=True, dtype=tf.float32):\n",
    "            w_parser_p = tf.get_variable(\"w_parser_p\")\n",
    "            w_parser_c = tf.get_variable(\"w_parser_c\")\n",
    "            b_parser_p = tf.get_variable(\"bias_parser_p\")\n",
    "            b_parser_c = tf.get_variable(\"bias_parser_c\")\n",
    "\n",
    "            w_parser_s = tf.get_variable(\"w_parser_s\")\n",
    "            w_parser_root = tf.get_variable(\"w_parser_root\")\n",
    "\n",
    "        parent = tf.tanh(tf.tensordot(input, w_parser_p, [[2], [0]]) + b_parser_p)\n",
    "        child = tf.tanh(tf.tensordot(input, w_parser_c, [[2], [0]])+b_parser_c)\n",
    "        # rep = LReLu(parent+child)\n",
    "        temp = tf.tensordot(parent,w_parser_s,[[-1],[0]])\n",
    "        raw_scores_words_ = tf.matmul(temp,tf.matrix_transpose(child))\n",
    "\n",
    "        # raw_scores_words_ = tf.squeeze(tf.tensordot(rep, w_parser_s, [[3], [0]]) , [3])\n",
    "        raw_scores_root_ = tf.squeeze(tf.tensordot(input, w_parser_root, [[2], [0]]) , [2])\n",
    "        raw_scores_words = tf.exp(raw_scores_words_)\n",
    "        raw_scores_root = tf.exp(raw_scores_root_)\n",
    "        tmp = tf.zeros_like(raw_scores_words[:,:,0])\n",
    "        raw_scores_words = tf.matrix_set_diag(raw_scores_words,tmp)\n",
    "\n",
    "        str_scores, LL = _getMatrixTree(raw_scores_root, raw_scores_words, mask1, mask2)\n",
    "        return str_scores\n",
    "\n",
    "    def _getMatrixTree(r, A, mask1, mask2):\n",
    "        L = tf.reduce_sum(A, 1)\n",
    "        L = tf.matrix_diag(L)\n",
    "        L = L - A\n",
    "\n",
    "        LL = L[:, 1:, :]\n",
    "        LL = tf.concat([tf.expand_dims(r, [1]), LL], 1)\n",
    "        LL_inv = tf.matrix_inverse(LL)  #batch_l, doc_l, doc_l\n",
    "        d0 = tf.multiply(r, LL_inv[:, :, 0])\n",
    "        LL_inv_diag = tf.expand_dims(tf.matrix_diag_part(LL_inv), 2)\n",
    "        tmp1 = tf.matrix_transpose(tf.multiply(tf.matrix_transpose(A), LL_inv_diag))\n",
    "        tmp2 = tf.multiply(A, tf.matrix_transpose(LL_inv))\n",
    "        d = mask1 * tmp1 - mask2 * tmp2\n",
    "        d = tf.concat([tf.expand_dims(d0,[1]), d], 1)\n",
    "        return d, LL\n",
    "\n",
    "    str_scores = _getDep(input, mask_parser_1, mask_parser_2)\n",
    "    return str_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self):\n",
    "        self.token_idxs = None\n",
    "        self.abstract_idxs = None\n",
    "        self.idx = -1\n",
    "\n",
    "    def _doc_len(self):\n",
    "        k = len(self.token_idxs)\n",
    "        return (k)\n",
    "\n",
    "    def _abstract_len(self):\n",
    "        k = len(self.abstract_idxs)\n",
    "        return k\n",
    "\n",
    "    def _max_ans_len(self):\n",
    "        k = max([len(ans) for ans in self.token_idxs])\n",
    "        return int(k)\n",
    "    \n",
    "    def _max_sent_len(self):\n",
    "        k = max([len(sent) for ans in self.token_idxs for sent in ans ])\n",
    "        return int(k)\n",
    "    \n",
    "class DataSet:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_examples = len(self.data)\n",
    "\n",
    "    def sort(self):\n",
    "        random.shuffle(self.data)\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_sent_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_ans_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._doc_len())\n",
    "\n",
    "    def get_by_idxs(self, idxs):\n",
    "        return [self.data[idx] for idx in idxs]\n",
    "\n",
    "    def get_batches(self, batch_size, num_epochs=None, rand = True):\n",
    "        num_batches_per_epoch = int(math.ceil(self.num_examples / batch_size))\n",
    "        idxs = list(range(self.num_examples))\n",
    "        _grouped = lambda: list(grouper(idxs, batch_size))\n",
    "\n",
    "        if(rand):\n",
    "            grouped = lambda: random.sample(_grouped(), num_batches_per_epoch)\n",
    "        else:\n",
    "            grouped = _grouped\n",
    "        num_steps = num_epochs*num_batches_per_epoch\n",
    "        batch_idx_tuples = itertools.chain.from_iterable(grouped() for _ in range(num_epochs))\n",
    "        for i in range(num_steps):\n",
    "            batch_idxs = tuple(i for i in next(batch_idx_tuples) if i is not None)\n",
    "            batch_data = self.get_by_idxs(batch_idxs)\n",
    "            yield i,batch_data\n",
    "\n",
    "class Params:\n",
    "    def __init__(self,n_embed,d_embed,vocab,vsize,dim_hidden):\n",
    "        self.n_embed = n_embed\n",
    "        self.d_embed = d_embed\n",
    "        self.vocab = vocab\n",
    "        self.vsize = vsize\n",
    "        self.dim_hidden = dim_hidden\n",
    "            \n",
    "class StructureModel():\n",
    "    def __init__(self, config,params):\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "        \n",
    "        t_variables = {}\n",
    "        t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        t_variables['batch_l'] = tf.placeholder(tf.int32)\n",
    "        \n",
    "        #Placeholder for answers and abstracts\n",
    "        t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None, None])\n",
    "        t_variables['abstract_idxs'] = tf.placeholder(tf.int32, [None,None])\n",
    "\n",
    "        #Storing length of each heirarchy element\n",
    "        t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None,None])\n",
    "        t_variables['ans_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "        t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "        t_variables['abstract_l'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        #Storing upper limit of each element length\n",
    "        t_variables['max_sent_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_doc_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_ans_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_abstract_l'] = tf.placeholder(tf.int32)\n",
    "\n",
    "        #Masks to limit element sizes\n",
    "        t_variables['mask_tokens'] = tf.placeholder(tf.float32, [None, None, None,None])\n",
    "        t_variables['mask_sents'] = tf.placeholder(tf.float32, [None, None,None])\n",
    "        t_variables['mask_answers']= tf.placeholder(tf.float32,[None,None])\n",
    "        t_variables['mask_abstracts'] = tf.placeholder(tf.float32,[None,None])\n",
    "        \n",
    "        #Parser Masks\n",
    "        t_variables['mask_parser_1'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "        t_variables['mask_parser_2'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "\n",
    "        t_variables['start_tokens'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        \n",
    "        self.t_variables = t_variables\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        abstracts_l_matrix = np.zeros([batch_size],np.int32)\n",
    "        doc_l_matrix = np.zeros([batch_size], np.int32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_ans = len(instance.token_idxs)\n",
    "            n_words = len(instance.abstract_idxs)\n",
    "            print('length of abstract',n_words)\n",
    "            doc_l_matrix[i] = n_ans\n",
    "            abstracts_l_matrix[i] = n_words\n",
    "        \n",
    "        max_doc_l = np.max(doc_l_matrix)\n",
    "        max_ans_l = max([max([len(ans) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_sent_l = max([max([max([len(sent) for itr in doc.token_idxs for sent in itr]) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_abstract_l = np.max(abstracts_l_matrix)\n",
    "\n",
    "        ans_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "        sent_l_matrix = np.zeros([batch_size, max_doc_l, max_ans_l], np.int32)\n",
    "\n",
    "        token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_ans_l, max_sent_l], np.int32)\n",
    "        abstract_idx_matrix = np.zeros([batch_size,max_abstract_l], np.int32)\n",
    "\n",
    "        mask_tokens_matrix = np.ones([batch_size, max_doc_l, max_ans_l, max_sent_l], np.float32)\n",
    "        mask_sents_matrix = np.ones([batch_size, max_doc_l, max_ans_l], np.float32)\n",
    "        mask_answers_matrix = np.ones([batch_size, max_doc_l],np.float32)\n",
    "        mask_abstact_matrix = np.ones([batch_size,max_abstract_l],np.float32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_answers = len(instance.token_idxs)\n",
    "            abstract_ = instance.abstract_idxs\n",
    "            abstract_idx_matrix[i,:len(abstract_)] = np.asarray(abstract_)\n",
    "            mask_abstact_matrix[i,len(abstract_):] = 0\n",
    "            abstracts_l_matrix[i] = len(abstract_)\n",
    "\n",
    "            for j, ans in enumerate(instance.token_idxs):\n",
    "                for k, sent in enumerate(instance.token_idxs[j]):\n",
    "                    token_idxs_matrix[i, j, k,:len(sent)] = np.asarray(sent)\n",
    "                    mask_tokens_matrix[i, j, k,len(sent):] = 0\n",
    "                    sent_l_matrix[i, j,k] = len(sent)\n",
    "\n",
    "                mask_sents_matrix[i,j,len(ans):]=0\n",
    "                ans_l_matrix[i,j] = len(ans)\n",
    "\n",
    "            mask_answers_matrix[i, n_answers:] = 0\n",
    "        \n",
    "        mask_parser_1 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_2 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_1[:, :, 0] = 0\n",
    "        mask_parser_2[:, 0, :] = 0\n",
    "        \n",
    "        feed_dict = {self.t_variables['token_idxs']: token_idxs_matrix,self.t_variables['abstract_idxs']: abstract_idx_matrix,\n",
    "                     self.t_variables['sent_l']: sent_l_matrix,self.t_variables['ans_l']:ans_l_matrix,self.t_variables['doc_l']: doc_l_matrix, \n",
    "                     self.t_variables['abstract_l']:abstracts_l_matrix,\n",
    "                     self.t_variables['mask_tokens']: mask_tokens_matrix, self.t_variables['mask_sents']: mask_sents_matrix, self.t_variables['mask_answers']:mask_answers_matrix, \n",
    "                     self.t_variables['mask_abstracts']: mask_abstact_matrix,\n",
    "                     self.t_variables['max_sent_l']: max_sent_l,self.t_variables['max_ans_l']:max_ans_l, self.t_variables['max_doc_l']: max_doc_l,\n",
    "                     self.t_variables['max_abstract_l']: max_abstract_l,\n",
    "                     self.t_variables['mask_parser_1']: mask_parser_1, self.t_variables['mask_parser_2']: mask_parser_2,\n",
    "                     self.t_variables['batch_l']: batch_size, self.t_variables['keep_prob']:self.config.keep_prob}\n",
    "\n",
    "        return  feed_dict\n",
    "\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(\"Embeddings\"):\n",
    "            #Initial embedding placeholders\n",
    "            self.embeddings = tf.get_variable(\"emb\", [self.params.n_embed, self.params.d_embed], dtype=tf.float32,\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root = tf.get_variable(\"emb_root\", [1, 1, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_a = tf.get_variable(\"emb_root_ans\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_s = tf.get_variable(\"emb_root_s\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Model\"):\n",
    "            #Weights and biases at pooling layers and final softmax for output. (Fianl might not be required)(Semantic combination part)\n",
    "            w_comb = tf.get_variable(\"w_comb\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb = tf.get_variable(\"bias_comb\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_a = tf.get_variable(\"w_comb_a\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_a = tf.get_variable(\"bias_comb_a\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_s = tf.get_variable(\"w_comb_s\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_s = tf.get_variable(\"bias_comb_s\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_softmax = tf.get_variable(\"w_softmax\", [2 * self.config.dim_sem, self.config.dim_output], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_softmax = tf.get_variable(\"bias_softmax\", [self.config.dim_output], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/doc\"):\n",
    "            #Placeholders for hierarchical model at document level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/ans\"):\n",
    "            #Placeholders for  hierarchial model at answer level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/sent\"):\n",
    "            #Placeholders for hierarchial model at sentence level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        #Variables of dimension batchsize passing length of each vector to architectures        \n",
    "        sent_l = self.t_variables['sent_l']\n",
    "        ans_l = self.t_variables['ans_l']\n",
    "        doc_l = self.t_variables['doc_l']\n",
    "        abstract_l = self.t_variables['abstract_l']\n",
    "        \n",
    "        #Maximum lengths of sentences, answers and documents to be processed\n",
    "        max_sent_l = self.t_variables['max_sent_l']\n",
    "        max_ans_l = self.t_variables['max_ans_l']\n",
    "        max_doc_l = self.t_variables['max_doc_l']\n",
    "        max_abstract_l = self.t_variables['max_abstract_l']\n",
    "\n",
    "        #batch size\n",
    "        batch_l = self.t_variables['batch_l']\n",
    "\n",
    "        #Creating embedding matrices for answers and abstracts corresponding to indexes\n",
    "        tokens_input = tf.nn.embedding_lookup(self.embeddings, self.t_variables['token_idxs'][:,:max_doc_l, :max_ans_l, :max_sent_l])\n",
    "        reference_input = tf.nn.embedding_lookup(self.embeddings,self.t_variables['abstract_idxs'][:,:max_abstract_l])\n",
    "        \n",
    "        #Dropout on input\n",
    "        tokens_input = tf.nn.dropout(tokens_input, self.t_variables['keep_prob'])\n",
    "\n",
    "        #Masking inputs\n",
    "        mask_tokens = self.t_variables['mask_tokens'][:,:max_doc_l, :max_ans_l, :max_sent_l]\n",
    "        mask_sents = self.t_variables['mask_sents'][:, :max_doc_l,:max_ans_l]\n",
    "        mask_answers = self.t_variables['mask_answers'][:,:max_doc_l]\n",
    "        mask_abstract = self.t_variables['mask_abstracts'][:,:max_abstract_l]\n",
    "\n",
    "\n",
    "        [_, _, _, _, rnn_size] = tokens_input.get_shape().as_list()\n",
    "        tokens_input_do = tf.reshape(tokens_input, [batch_l * max_doc_l*max_ans_l, max_sent_l, rnn_size])\n",
    "\n",
    "        sent_l = tf.reshape(sent_l, [batch_l * max_doc_l* max_ans_l])\n",
    "        mask_tokens = tf.reshape(mask_tokens, [batch_l * max_doc_l*max_ans_l, -1])\n",
    "\n",
    "        #Word level input\n",
    "        tokens_output, _ = dynamicBiRNN(tokens_input_do, sent_l, n_hidden=self.params.dim_hidden,\n",
    "                                        cell_type=self.config.rnn_cell, cell_name='Model/sent')\n",
    "        \n",
    "        tokens_sem = tf.concat([tokens_output[0][:,:,:self.config.dim_sem], tokens_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        tokens_str = tf.concat([tokens_output[0][:,:,self.config.dim_sem:], tokens_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "        \n",
    "        temp1 = tf.zeros([batch_l * max_doc_l*max_ans_l, max_sent_l,1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l*max_ans_l ,1,max_sent_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l, max_sent_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l-1, max_sent_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_s_ = get_structure('sent', tokens_str, max_sent_l, mask1, mask2)  # batch_l,  sent_l+1, sent_l\n",
    "        str_scores_s = tf.matrix_transpose(str_scores_s_)  # soft parent\n",
    "        tokens_sem_root = tf.concat([tf.tile(embeddings_root_s, [batch_l * max_doc_l *max_ans_l, 1, 1]), tokens_sem], 1)\n",
    "        tokens_output_ = tf.matmul(str_scores_s, tokens_sem_root)\n",
    "        tokens_output = LReLu(tf.tensordot(tf.concat([tokens_sem, tokens_output_], 2), w_comb_s, [[2], [0]]) + b_comb_s)\n",
    "\n",
    "        if (self.config.sent_attention == 'sum'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)\n",
    "        elif (self.config.sent_attention == 'mean'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)/tf.expand_dims(tf.cast(sent_l,tf.float32),1)\n",
    "        elif (self.config.sent_attention == 'max'):\n",
    "            tokens_output = tokens_output + tf.expand_dims((mask_tokens-1)*999,2)\n",
    "            tokens_output = tf.reduce_max(tokens_output, 1)\n",
    "\n",
    "        #Sentence level RNN\n",
    "        sents_input = tf.reshape(tokens_output, [batch_l*max_doc_l, max_ans_l,2*self.config.dim_sem])\n",
    "        ans_l = tf.reshape(ans_l,[batch_l*max_doc_l])\n",
    "        mask_sents = tf.reshape(mask_sents,[batch_l*max_doc_l,-1])\n",
    "\n",
    "        sents_output, _ = dynamicBiRNN(sents_input, ans_l, n_hidden=self.params.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/ans')\n",
    "\n",
    "        sents_sem = tf.concat([sents_output[0][:,:,:self.config.dim_sem], sents_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        sents_str = tf.concat([sents_output[0][:,:,self.config.dim_sem:], sents_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        temp1 = tf.zeros([batch_l * max_doc_l, max_ans_l, 1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l, 1, max_ans_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l , max_ans_l, max_ans_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l , max_ans_l-1, max_ans_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_ = get_structure('ans', sents_str, max_ans_l, mask1,mask2)  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        sents_sem_root = tf.concat([tf.tile(embeddings_root_a, [batch_l*max_doc_l, 1, 1]), sents_sem], 1)\n",
    "        sents_output_ = tf.matmul(str_scores, sents_sem_root)\n",
    "        sents_output = LReLu(tf.tensordot(tf.concat([sents_sem, sents_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.doc_attention == 'sum'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)\n",
    "        elif (self.config.doc_attention == 'mean'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)/tf.expand_dims(tf.cast(ans_l,tf.float32),1)\n",
    "        elif (self.config.doc_attention == 'max'):\n",
    "            sents_output = sents_output + tf.expand_dims((mask_sents-1)*999,2)\n",
    "            sents_output = tf.reduce_max(sents_output, 1)\n",
    "\n",
    "        #Answer level RNN\n",
    "        ans_input = tf.reshape(sents_output, [batch_l, max_doc_l,2*self.config.dim_sem])\n",
    "        ans_output, _ = dynamicBiRNN(ans_input, doc_l, n_hidden=self.params.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/doc')\n",
    "\n",
    "        ans_sem = tf.concat([ans_output[0][:,:,:self.config.dim_sem], ans_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        ans_str = tf.concat([ans_output[0][:,:,self.config.dim_sem:], ans_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        str_scores_ = get_structure('doc', ans_str, max_doc_l, self.t_variables['mask_parser_1'], self.t_variables['mask_parser_2'])  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        ans_sem_root = tf.concat([tf.tile(embeddings_root, [batch_l, 1, 1]), ans_sem], 1)\n",
    "        ans_output_ = tf.matmul(str_scores, ans_sem_root)\n",
    "        ans_output = LReLu(tf.tensordot(tf.concat([ans_sem, ans_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.ans_attention == 'sum'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            # ans_output = tf.reduce_sum(ans_output, 1)\n",
    "        elif (self.config.ans_attention == 'mean'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            ans_output = tf.reduce_sum(ans_output, 1)/tf.expand_dims(tf.cast(doc_l,tf.float32),1)\n",
    "        elif (self.config.ans_attention == 'max'):\n",
    "            ans_output = ans_output + tf.expand_dims((mask_answers-1)*999,2)\n",
    "            ans_output = tf.reduce_max(ans_output, 1)\n",
    "\n",
    "        encoder_output = ans_output\n",
    "        print(\"Encoder pooled ans_output shape\", encoder_output.shape)\n",
    "\n",
    "        tgt_vocab_size = self.params.vsize\n",
    "        learning_rate = self.config.lr\n",
    "        \n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(self.config.dim_output)\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(reference_input, abstract_l, time_major=False)\n",
    "        lstm_init = tf.contrib.rnn.LSTMStateTuple(encoder_output,encoder_output)\n",
    "        projection_layer = tf.layers.Dense(tgt_vocab_size, use_bias=False)\n",
    "        \n",
    "        #How to initialize the LSTM?\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,initial_state=lstm_init,output_layer=projection_layer)\n",
    "        \n",
    "        outputs, states, seq_l = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        print(\"logits shaped [batch x seq_len x decoder_vocabulary_size] \", logits.shape)\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.t_variables['abstract_idxs'], logits=logits)\n",
    "        target_weights = tf.sequence_mask(abstract_l, max_abstract_l, dtype=tf.float32)\n",
    "        reduced_loss = tf.reduce_sum(loss*target_weights)/tf.to_float(batch_l)\n",
    "        \n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,epsilon=0.1)\n",
    "        update_step = optimizer.apply_gradients(zip(clipped_gradients, params),global_step=global_step)\n",
    "\n",
    "        self.final_output = logits\n",
    "        self.loss = reduced_loss\n",
    "        self.opt = optimizer.minimize(loss)\n",
    "\n",
    "#         inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_decoder,tf.fill([hparams.batch_size], tgt_sos_id), tgt_eos_id)\n",
    "#         inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, inference_helper, initial_state,output_layer=projection_layer)\n",
    "#         source_sequence_length = hparams.encoder_lengthmaximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n",
    "#         outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, maximum_iterations=maximum_iterations)translations = outputs.sample_id\n",
    "\n",
    "#         decoder_initial_state = tf.contrib.seq2seq.tile_batch(initial_state, multiplier=hparams.beam_width)\n",
    "#         inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,embedding=embedding_decoder,start_tokens=tf.fill([hparams.batch_size], tgt_sos_id),\n",
    "#         end_token=tgt_eos_id,initial_state=decoder_initial_state,beam_width=hparams.beam_width,output_layer=projection_layer,\n",
    "#         length_penalty_weight=0.0)\n",
    "#         outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, maximum_iterations=maximum_iterations)\n",
    "#         translations = outputs.predicted_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded succesfully\n"
     ]
    }
   ],
   "source": [
    "#Main function begins here\n",
    "config = flags.FLAGS\n",
    "\n",
    "remaining_args = flags.FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
    "assert(remaining_args == [sys.argv[0]])\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu)\n",
    "\n",
    "hash = random.getrandbits(32)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ah = logging.FileHandler(str(hash)+'.log')\n",
    "ah.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "ah.setFormatter(formatter)\n",
    "logger.addHandler(ah)\n",
    "\n",
    "gc.disable()\n",
    "train, dev, test, embeddings, vocab = pickle.load(open(config.data_file,'rb'))\n",
    "gc.enable()\n",
    "print('Data loaded succesfully')\n",
    "\n",
    "trainset, devset, testset = DataSet(train), DataSet(dev), DataSet(test)\n",
    "vocab = dict([(v.index,k) for k,v in vocab.items()])\n",
    "trainset.sort()\n",
    "train_batches = trainset.get_batches(config.batch_size, config.epochs, rand=True)\n",
    "dev_batches = devset.get_batches(config.batch_size, 1, rand=False)\n",
    "test_batches = testset.get_batches(config.batch_size, 1, rand=False)\n",
    "dev_batches = [i for i in dev_batches]\n",
    "test_batches = [i for i in test_batches]\n",
    "\n",
    "num_examples, train_batches, dev_batches, test_batches, embedding_matrix, vocab = len(train), train_batches, dev_batches, test_batches, embeddings, vocab\n",
    "\n",
    "n_embed,d_embed = embedding_matrix.shape\n",
    "vsize = len(vocab)\n",
    "# config.inv_vocab = {v: k for k, v in vocab.items()}\n",
    "dim_hidden = config.dim_sem+config.dim_str\n",
    "params = Params(n_embed,d_embed,vocab,vsize,dim_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StructureModel(config,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder pooled ans_output shape (?, 150)\n",
      "logits shaped [batch x seq_len x decoder_vocabulary_size]  (?, ?, 226129)\n"
     ]
    }
   ],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/114120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 99\n",
      "length of abstract 87\n",
      "length of abstract 101\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 98\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 93\n",
      "length of abstract 99\n",
      "length of abstract 97\n",
      "length of abstract 100\n",
      "Loss in 0  is:  1213.0032958984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/114120 [00:36<1146:02:05, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 101\n",
      "length of abstract 107\n",
      "length of abstract 100\n",
      "length of abstract 101\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 108\n",
      "length of abstract 100\n",
      "length of abstract 101\n",
      "length of abstract 101\n",
      "length of abstract 100\n",
      "length of abstract 101\n",
      "length of abstract 101\n",
      "length of abstract 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/114120 [01:06<1089:37:13, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of abstract 111\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 103\n",
      "length of abstract 102\n",
      "length of abstract 100\n",
      "length of abstract 99\n",
      "length of abstract 105\n",
      "length of abstract 100\n",
      "length of abstract 100\n",
      "length of abstract 107\n",
      "length of abstract 99\n",
      "length of abstract 100\n",
      "length of abstract 99\n",
      "length of abstract 100\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch = int(num_examples / config.batch_size)\n",
    "num_steps = config.epochs * num_batches_per_epoch\n",
    "\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tfconfig) as sess:\n",
    "    gvi = tf.global_variables_initializer()\n",
    "    sess.run(gvi)\n",
    "    sess.run(model.embeddings.assign(embedding_matrix.astype(np.float32)))\n",
    "    loss = 0\n",
    "\n",
    "    for ct, batch in tqdm.tqdm(train_batches, total=num_steps):\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        outputs,_,_loss = sess.run([model.final_output, model.opt, model.loss], feed_dict=feed_dict)\n",
    "        loss+=_loss\n",
    "        if(ct%config.log_period==0):\n",
    "            print ('Loss in',ct,' is: ',loss)\n",
    "            model_name = 'Checkpoints/model'+str(ct)+'.ckpt'\n",
    "            save_path = saver.save(sess, model_name)\n",
    "            loss = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
