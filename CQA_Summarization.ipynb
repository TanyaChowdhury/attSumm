{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import tqdm\n",
    "import gc\n",
    "import math\n",
    "import unicodedata\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "from six.moves import zip_longest\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string(\"mode\",\"infer\",\"mode\")\n",
    "flags.DEFINE_string(\"inference_mode\",'greedy',\"inference_mode\")\n",
    "\n",
    "flags.DEFINE_string(\"rnn_cell\", \"lstm\", \"rnn cell\")\n",
    "flags.DEFINE_string(\"data_file\", \"Data/CQA_codes.pkl\", \"data_file\")\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 3, \"batch_size\")\n",
    "flags.DEFINE_integer(\"epochs\", 30, \"epochs\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_summary_length\",100,\"max_summary_length\")\n",
    "\n",
    "flags.DEFINE_integer(\"dim_str\", 50, \"dim_str\")\n",
    "flags.DEFINE_integer(\"dim_sem\", 75, \"dim_sem\")\n",
    "flags.DEFINE_integer(\"dim_output\", 150, \"dim_output\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.7, \"keep_prob\")\n",
    "flags.DEFINE_float(\"lr\", 0.02, \"lr\")\n",
    "flags.DEFINE_float(\"norm\", 1e-4, \"norm\")\n",
    "flags.DEFINE_integer(\"gpu\", 0, \"gpu\")\n",
    "\n",
    "flags.DEFINE_string(\"sent_attention\", \"max\", \"sent_attention\")\n",
    "flags.DEFINE_string(\"ans_attention\", \"max\", \"ans_attention\")\n",
    "flags.DEFINE_string(\"doc_attention\", \"max\", \"doc_attention\")\n",
    "flags.DEFINE_bool(\"large_data\", True, \"large_data\")\n",
    "flags.DEFINE_integer(\"log_period\", 100, \"log_period\")\n",
    "flags.DEFINE_integer(\"beam_width\",4,\"beam_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None, shorten=False, num_groups=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue,) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = (tuple(e for e in each if e is not None) for each in out)\n",
    "    return out\n",
    "\n",
    "def LReLu(x, leak=0.01):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * tf.abs(x)\n",
    "\n",
    "def dynamicBiRNN(input, seqlen, n_hidden, cell_type, cell_name=''):\n",
    "    batch_size = tf.shape(input)[0]\n",
    "    with tf.variable_scope(cell_name + 'fw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            fw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            fw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "\n",
    "        fw_initial_state = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    with tf.variable_scope(cell_name + 'bw', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32):\n",
    "        if(cell_type == 'gru'):\n",
    "            bw_cell = tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        elif(cell_type == 'lstm'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "        bw_initial_state = bw_cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(cell_name):\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, input,\n",
    "                                                                 initial_state_fw=fw_initial_state,\n",
    "                                                                 initial_state_bw=bw_initial_state,\n",
    "                                                                 sequence_length=seqlen)\n",
    "    return outputs, output_states\n",
    "\n",
    "def decode(helper, scope, reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units, memory=encoder_outputs,memory_sequence_length=input_lengths)\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "        attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(attn_cell, vocab_size, reuse=reuse)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell=out_cell, helper=helper,initial_state=out_cell.zero_state(dtype=tf.float32, batch_size=batch_size))#initial_state=encoder_final_state)\n",
    "        outputs = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, output_time_major=False,impute_finished=True, maximum_iterations=self.config.max_summary_length)\n",
    "        return outputs[0]\n",
    "    \n",
    "def get_structure(name, input, max_l, mask_parser_1, mask_parser_2):\n",
    "    def _getDep(input, mask1, mask2):\n",
    "        #input: batch_l, sent_l, rnn_size\n",
    "        with tf.variable_scope(\"Structure/\"+name, reuse=True, dtype=tf.float32):\n",
    "            w_parser_p = tf.get_variable(\"w_parser_p\")\n",
    "            w_parser_c = tf.get_variable(\"w_parser_c\")\n",
    "            b_parser_p = tf.get_variable(\"bias_parser_p\")\n",
    "            b_parser_c = tf.get_variable(\"bias_parser_c\")\n",
    "\n",
    "            w_parser_s = tf.get_variable(\"w_parser_s\")\n",
    "            w_parser_root = tf.get_variable(\"w_parser_root\")\n",
    "\n",
    "        parent = tf.tanh(tf.tensordot(input, w_parser_p, [[2], [0]]) + b_parser_p)\n",
    "        child = tf.tanh(tf.tensordot(input, w_parser_c, [[2], [0]])+b_parser_c)\n",
    "        # rep = LReLu(parent+child)\n",
    "        temp = tf.tensordot(parent,w_parser_s,[[-1],[0]])\n",
    "        raw_scores_words_ = tf.matmul(temp,tf.matrix_transpose(child))\n",
    "\n",
    "        # raw_scores_words_ = tf.squeeze(tf.tensordot(rep, w_parser_s, [[3], [0]]) , [3])\n",
    "        raw_scores_root_ = tf.squeeze(tf.tensordot(input, w_parser_root, [[2], [0]]) , [2])\n",
    "        raw_scores_words = tf.exp(raw_scores_words_)\n",
    "        raw_scores_root = tf.exp(raw_scores_root_)\n",
    "        tmp = tf.zeros_like(raw_scores_words[:,:,0])\n",
    "        raw_scores_words = tf.matrix_set_diag(raw_scores_words,tmp)\n",
    "\n",
    "        str_scores, LL = _getMatrixTree(raw_scores_root, raw_scores_words, mask1, mask2)\n",
    "        return str_scores\n",
    "\n",
    "    def _getMatrixTree(r, A, mask1, mask2):\n",
    "        L = tf.reduce_sum(A, 1)\n",
    "        L = tf.matrix_diag(L)\n",
    "        L = L - A\n",
    "\n",
    "        LL = L[:, 1:, :]\n",
    "        LL = tf.concat([tf.expand_dims(r, [1]), LL], 1)\n",
    "        LL_inv = tf.matrix_inverse(LL)  #batch_l, doc_l, doc_l\n",
    "        d0 = tf.multiply(r, LL_inv[:, :, 0])\n",
    "        LL_inv_diag = tf.expand_dims(tf.matrix_diag_part(LL_inv), 2)\n",
    "        tmp1 = tf.matrix_transpose(tf.multiply(tf.matrix_transpose(A), LL_inv_diag))\n",
    "        tmp2 = tf.multiply(A, tf.matrix_transpose(LL_inv))\n",
    "        d = mask1 * tmp1 - mask2 * tmp2\n",
    "        d = tf.concat([tf.expand_dims(d0,[1]), d], 1)\n",
    "        return d, LL\n",
    "\n",
    "    str_scores = _getDep(input, mask_parser_1, mask_parser_2)\n",
    "    return str_scores\n",
    "\n",
    "\n",
    "def initialize_uninitialized_vars(sess):\n",
    "    from itertools import compress\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([~(tf.is_variable_initialized(var)) \\\n",
    "                                   for var in global_vars])\n",
    "    not_initialized_vars = list(compress(global_vars, is_not_initialized))\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self):\n",
    "        self.token_idxs = None\n",
    "        self.abstract_idxs = None\n",
    "        self.idx = -1\n",
    "\n",
    "    def _doc_len(self):\n",
    "        k = len(self.token_idxs)\n",
    "        return (k)\n",
    "\n",
    "    def _abstract_len(self):\n",
    "        k = len(self.abstract_idxs)\n",
    "        return k\n",
    "\n",
    "    def _max_ans_len(self):\n",
    "        k = max([len(ans) for ans in self.token_idxs])\n",
    "        return int(k)\n",
    "    \n",
    "    def _max_sent_len(self):\n",
    "        k = max([len(sent) for ans in self.token_idxs for sent in ans ])\n",
    "        return int(k)\n",
    "    \n",
    "class DataSet:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_examples = len(self.data)\n",
    "\n",
    "    def sort(self):\n",
    "        random.shuffle(self.data)\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_sent_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._max_ans_len())\n",
    "        self.data = sorted(self.data, key=lambda x: x._doc_len())\n",
    "\n",
    "    def get_by_idxs(self, idxs):\n",
    "        return [self.data[idx] for idx in idxs]\n",
    "\n",
    "    def get_batches(self, batch_size, num_epochs=None, rand = True):\n",
    "        num_batches_per_epoch = int(math.ceil(self.num_examples / batch_size))\n",
    "        idxs = list(range(self.num_examples))\n",
    "        _grouped = lambda: list(grouper(idxs, batch_size))\n",
    "\n",
    "        if(rand):\n",
    "            grouped = lambda: random.sample(_grouped(), num_batches_per_epoch)\n",
    "        else:\n",
    "            grouped = _grouped\n",
    "        num_steps = num_epochs*num_batches_per_epoch\n",
    "        batch_idx_tuples = itertools.chain.from_iterable(grouped() for _ in range(num_epochs))\n",
    "        for i in range(num_steps):\n",
    "            batch_idxs = tuple(i for i in next(batch_idx_tuples) if i is not None)\n",
    "            batch_data = self.get_by_idxs(batch_idxs)\n",
    "            yield i,batch_data\n",
    "\n",
    "class Params:\n",
    "    def __init__(self,n_embed,d_embed,vocab,inv_vocab,vsize,dim_hidden,embeddings):\n",
    "        self.n_embed = n_embed\n",
    "        self.d_embed = d_embed\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = inv_vocab\n",
    "        self.vsize = vsize\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.embeddings = embeddings.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs, initial_state, encoder_states, cell):\n",
    "    with variable_scope.variable_scope(\"attention_decoder\") as scope:\n",
    "        batch_size = encoder_states.get_shape()[0].value\n",
    "        attn_size = encoder_states.get_shape()[2].value\n",
    "        \n",
    "        encoder_states = tf.expand_dims(encoder_states, axis=2)\n",
    "        attention_vec_size = attn_size\n",
    "        W_h = variable_scope.get_variable(\"W_h\", [1, 1, attn_size, attention_vec_size])\n",
    "        encoder_features = nn_ops.conv2d(encoder_states, W_h, [1, 1, 1, 1], \"SAME\")\n",
    "        v = variable_scope.get_variable(\"v\", [attention_vec_size])\n",
    "        \n",
    "        def attention(decoder_state):\n",
    "            with variable_scope.variable_scope(\"Attention\"):\n",
    "                decoder_features = linear(decoder_state, attention_vec_size, True)\n",
    "                decoder_features = tf.expand_dims(tf.expand_dims(decoder_features, 1), 1)\n",
    "                e = math_ops.reduce_sum(v * math_ops.tanh(encoder_features + decoder_features), [2, 3])\n",
    "                attn_dist = nn_ops.softmax(e)\n",
    "                masked_sums = tf.reduce_sum(attn_dist, axis=1)\n",
    "                attn_dist = attn_dist / tf.reshape(masked_sums, [-1, 1])\n",
    "                context_vector = math_ops.reduce_sum(array_ops.reshape(attn_dist, [batch_size, -1, 1, 1]) * encoder_states, [1, 2]) # shape (batch_size, attn_size).\n",
    "                context_vector = array_ops.reshape(context_vector, [-1, attn_size])\n",
    "            return context_vector, attn_dist\n",
    "        \n",
    "        def linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "            total_arg_size = 0\n",
    "            shapes = [a.get_shape().as_list() for a in args]\n",
    "            for shape in shapes:\n",
    "                total_arg_size += shape[1]\n",
    "            with tf.variable_scope(scope or \"Linear\"):\n",
    "                matrix = tf.get_variable(\"Matrix\", [total_arg_size, output_size])\n",
    "                if len(args) == 1:\n",
    "                      res = tf.matmul(args[0], matrix)\n",
    "                else:\n",
    "                    res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n",
    "                if not bias:\n",
    "                    return res\n",
    "                bias_term = tf.get_variable(\n",
    "                        \"Bias\", [output_size], initializer=tf.constant_initializer(bias_start))\n",
    "            return res + bias_term\n",
    "      \n",
    "        outputs = []\n",
    "        attn_dists = []\n",
    "        p_gens = []\n",
    "        state = initial_state\n",
    "        context_vector = array_ops.zeros([batch_size, attn_size])\n",
    "        context_vector.set_shape([None, attn_size])\n",
    "        for i, inp in enumerate(decoder_inputs):\n",
    "            tf.logging.info(\"Adding attention_decoder timestep %i of %i\", i, len(decoder_inputs))\n",
    "            if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "            input_size = inp.get_shape().with_rank(2)[1]\n",
    "            x = linear([inp] + [context_vector], input_size, True)\n",
    "            cell_output, state = cell(x, state)\n",
    "            context_vector, attn_dist = attention(state)\n",
    "            attn_dists.append(attn_dist)\n",
    "            with tf.variable_scope('calculate_pgen'):\n",
    "                p_gen = linear([context_vector, state.c, state.h, x], 1, True)\n",
    "                p_gen = tf.sigmoid(p_gen)\n",
    "                p_gens.append(p_gen)\n",
    "            with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "                output = linear([cell_output] + [context_vector], cell.output_size, True)\n",
    "            outputs.append(output)\n",
    "    \n",
    "    return outputs, state, attn_dists, p_gens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureModel():\n",
    "    def __init__(self, config,params):\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "        \n",
    "        t_variables = {}\n",
    "        t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        t_variables['batch_l'] = tf.placeholder(tf.int32)\n",
    "        \n",
    "        #Placeholder for answers and abstracts\n",
    "        t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None, None])\n",
    "        t_variables['abstract_idxs'] = tf.placeholder(tf.int32, [None,None])\n",
    "\n",
    "        #Storing length of each heirarchy element\n",
    "        t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None,None])\n",
    "        t_variables['ans_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "        t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "        t_variables['abstract_l'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        #Storing upper limit of each element length\n",
    "        t_variables['max_sent_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_doc_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_ans_l'] = tf.placeholder(tf.int32)\n",
    "        t_variables['max_abstract_l'] = tf.placeholder(tf.int32)\n",
    "\n",
    "        #Masks to limit element sizes\n",
    "        t_variables['mask_tokens'] = tf.placeholder(tf.float32, [None, None, None,None])\n",
    "        t_variables['mask_sents'] = tf.placeholder(tf.float32, [None, None,None])\n",
    "        t_variables['mask_answers']= tf.placeholder(tf.float32,[None,None])\n",
    "        t_variables['mask_abstracts'] = tf.placeholder(tf.float32,[None,None])\n",
    "        \n",
    "        #Parser Masks\n",
    "        t_variables['mask_parser_1'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "        t_variables['mask_parser_2'] = tf.placeholder(tf.float32, [None, None, None])\n",
    "\n",
    "        t_variables['start_tokens'] = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "        \n",
    "        self.t_variables = t_variables\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        abstracts_l_matrix = np.zeros([batch_size],np.int32)\n",
    "        doc_l_matrix = np.zeros([batch_size], np.int32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_ans = len(instance.token_idxs)\n",
    "            n_words = len(instance.abstract_idxs)\n",
    "            doc_l_matrix[i] = n_ans\n",
    "            abstracts_l_matrix[i] = n_words\n",
    "        \n",
    "        max_doc_l = np.max(doc_l_matrix)\n",
    "        max_ans_l = max([max([len(ans) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_sent_l = max([max([max([len(sent) for itr in doc.token_idxs for sent in itr]) for ans in doc.token_idxs]) for doc in batch])\n",
    "        max_abstract_l = np.max(abstracts_l_matrix)\n",
    "\n",
    "        ans_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "        sent_l_matrix = np.zeros([batch_size, max_doc_l, max_ans_l], np.int32)\n",
    "\n",
    "        token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_ans_l, max_sent_l], np.int32)\n",
    "        abstract_idx_matrix = np.zeros([batch_size,max_abstract_l], np.int32)\n",
    "\n",
    "        mask_tokens_matrix = np.ones([batch_size, max_doc_l, max_ans_l, max_sent_l], np.float32)\n",
    "        mask_sents_matrix = np.ones([batch_size, max_doc_l, max_ans_l], np.float32)\n",
    "        mask_answers_matrix = np.ones([batch_size, max_doc_l],np.float32)\n",
    "        mask_abstact_matrix = np.ones([batch_size,max_abstract_l],np.float32)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            n_answers = len(instance.token_idxs)\n",
    "            abstract_ = instance.abstract_idxs\n",
    "            abstract_idx_matrix[i,:len(abstract_)] = np.asarray(abstract_)\n",
    "            mask_abstact_matrix[i,len(abstract_):] = 0\n",
    "            abstracts_l_matrix[i] = len(abstract_)\n",
    "\n",
    "            for j, ans in enumerate(instance.token_idxs):\n",
    "                for k, sent in enumerate(instance.token_idxs[j]):\n",
    "                    token_idxs_matrix[i, j, k,:len(sent)] = np.asarray(sent)\n",
    "                    mask_tokens_matrix[i, j, k,len(sent):] = 0\n",
    "                    sent_l_matrix[i, j,k] = len(sent)\n",
    "\n",
    "                mask_sents_matrix[i,j,len(ans):]=0\n",
    "                ans_l_matrix[i,j] = len(ans)\n",
    "\n",
    "            mask_answers_matrix[i, n_answers:] = 0\n",
    "        \n",
    "        mask_parser_1 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_2 = np.ones([batch_size, max_doc_l, max_doc_l], np.float32)\n",
    "        mask_parser_1[:, :, 0] = 0\n",
    "        mask_parser_2[:, 0, :] = 0\n",
    "        \n",
    "        feed_dict = {self.t_variables['token_idxs']: token_idxs_matrix,self.t_variables['abstract_idxs']: abstract_idx_matrix,\n",
    "                     self.t_variables['sent_l']: sent_l_matrix,self.t_variables['ans_l']:ans_l_matrix,self.t_variables['doc_l']: doc_l_matrix, \n",
    "                     self.t_variables['abstract_l']:abstracts_l_matrix,\n",
    "                     self.t_variables['mask_tokens']: mask_tokens_matrix, self.t_variables['mask_sents']: mask_sents_matrix, self.t_variables['mask_answers']:mask_answers_matrix, \n",
    "                     self.t_variables['mask_abstracts']: mask_abstact_matrix,\n",
    "                     self.t_variables['max_sent_l']: max_sent_l,self.t_variables['max_ans_l']:max_ans_l, self.t_variables['max_doc_l']: max_doc_l,\n",
    "                     self.t_variables['max_abstract_l']: max_abstract_l,\n",
    "                     self.t_variables['mask_parser_1']: mask_parser_1, self.t_variables['mask_parser_2']: mask_parser_2,\n",
    "                     self.t_variables['batch_l']: batch_size, self.t_variables['keep_prob']:self.config.keep_prob}\n",
    "\n",
    "        return  feed_dict\n",
    "\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(\"Embeddings\"):\n",
    "            #Initial embedding placeholders\n",
    "            self.embeddings = tf.get_variable(\"emb\", [self.params.n_embed, self.params.d_embed], dtype=tf.float32,\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root = tf.get_variable(\"emb_root\", [1, 1, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_a = tf.get_variable(\"emb_root_ans\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings_root_s = tf.get_variable(\"emb_root_s\", [1, 1,2* self.config.dim_sem], dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Model\"):\n",
    "            #Weights and biases at pooling layers and final softmax for output. (Fianl might not be required)(Semantic combination part)\n",
    "            w_comb = tf.get_variable(\"w_comb\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb = tf.get_variable(\"bias_comb\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_a = tf.get_variable(\"w_comb_a\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_a = tf.get_variable(\"bias_comb_a\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_comb_s = tf.get_variable(\"w_comb_s\", [4 * self.config.dim_sem, 2 * self.config.dim_sem], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_comb_s = tf.get_variable(\"bias_comb_s\", [2 * self.config.dim_sem], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "            w_softmax = tf.get_variable(\"w_softmax\", [2 * self.config.dim_sem, self.config.dim_output], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_softmax = tf.get_variable(\"bias_softmax\", [self.config.dim_output], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/doc\"):\n",
    "            #Placeholders for hierarchical model at document level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/ans\"):\n",
    "            #Placeholders for  hierarchial model at answer level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.variable_scope(\"Structure/sent\"):\n",
    "            #Placeholders for hierarchial model at sentence level(structural part)\n",
    "            tf.get_variable(\"w_parser_p\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_c\", [2 * self.config.dim_str, 2 * self.config.dim_str],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_p\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"bias_parser_c\", [2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            tf.get_variable(\"w_parser_s\", [2 * self.config.dim_str, 2 * self.config.dim_str], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "            tf.get_variable(\"w_parser_root\", [2 * self.config.dim_str, 1], dtype=tf.float32,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        #Variables of dimension batchsize passing length of each vector to architectures        \n",
    "        sent_l = self.t_variables['sent_l']\n",
    "        ans_l = self.t_variables['ans_l']\n",
    "        doc_l = self.t_variables['doc_l']\n",
    "        abstract_l = self.t_variables['abstract_l']\n",
    "        \n",
    "        #Maximum lengths of sentences, answers and documents to be processed\n",
    "        max_sent_l = self.t_variables['max_sent_l']\n",
    "        max_ans_l = self.t_variables['max_ans_l']\n",
    "        max_doc_l = self.t_variables['max_doc_l']\n",
    "        max_abstract_l = self.t_variables['max_abstract_l']\n",
    "\n",
    "        #batch size\n",
    "        batch_l = self.t_variables['batch_l']\n",
    "\n",
    "        #Creating embedding matrices for answers and abstracts corresponding to indexes\n",
    "        tokens_input = tf.nn.embedding_lookup(self.embeddings, self.t_variables['token_idxs'][:,:max_doc_l, :max_ans_l, :max_sent_l])\n",
    "        reference_input = tf.nn.embedding_lookup(self.embeddings,self.t_variables['abstract_idxs'][:,:max_abstract_l])\n",
    "        \n",
    "        #Dropout on input\n",
    "        tokens_input = tf.nn.dropout(tokens_input, self.t_variables['keep_prob'])\n",
    "\n",
    "        #Masking inputs\n",
    "        mask_tokens = self.t_variables['mask_tokens'][:,:max_doc_l, :max_ans_l, :max_sent_l]\n",
    "        mask_sents = self.t_variables['mask_sents'][:, :max_doc_l,:max_ans_l]\n",
    "        mask_answers = self.t_variables['mask_answers'][:,:max_doc_l]\n",
    "        mask_abstract = self.t_variables['mask_abstracts'][:,:max_abstract_l]\n",
    "\n",
    "\n",
    "        [_, _, _, _, rnn_size] = tokens_input.get_shape().as_list()\n",
    "        tokens_input_do = tf.reshape(tokens_input, [batch_l * max_doc_l*max_ans_l, max_sent_l, rnn_size])\n",
    "\n",
    "        sent_l = tf.reshape(sent_l, [batch_l * max_doc_l* max_ans_l])\n",
    "        mask_tokens = tf.reshape(mask_tokens, [batch_l * max_doc_l*max_ans_l, -1])\n",
    "\n",
    "        #Word level input\n",
    "        tokens_output, token_encoder_states = dynamicBiRNN(tokens_input_do, sent_l, n_hidden=self.params.dim_hidden,\n",
    "                                        cell_type=self.config.rnn_cell, cell_name='Model/sent')\n",
    "        \n",
    "        tokens_sem = tf.concat([tokens_output[0][:,:,:self.config.dim_sem], tokens_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        tokens_str = tf.concat([tokens_output[0][:,:,self.config.dim_sem:], tokens_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "        \n",
    "        temp1 = tf.zeros([batch_l * max_doc_l*max_ans_l, max_sent_l,1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l*max_ans_l ,1,max_sent_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l, max_sent_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l * max_ans_l, max_sent_l-1, max_sent_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_s_ = get_structure('sent', tokens_str, max_sent_l, mask1, mask2)  # batch_l,  sent_l+1, sent_l\n",
    "        str_scores_s = tf.matrix_transpose(str_scores_s_)  # soft parent\n",
    "        tokens_sem_root = tf.concat([tf.tile(embeddings_root_s, [batch_l * max_doc_l *max_ans_l, 1, 1]), tokens_sem], 1)\n",
    "        tokens_output_ = tf.matmul(str_scores_s, tokens_sem_root)\n",
    "        tokens_output = LReLu(tf.tensordot(tf.concat([tokens_sem, tokens_output_], 2), w_comb_s, [[2], [0]]) + b_comb_s)\n",
    "\n",
    "        if (self.config.sent_attention == 'sum'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)\n",
    "        elif (self.config.sent_attention == 'mean'):\n",
    "            tokens_output = tokens_output * tf.expand_dims(mask_tokens,2)\n",
    "            tokens_output = tf.reduce_sum(tokens_output, 1)/tf.expand_dims(tf.cast(sent_l,tf.float32),1)\n",
    "        elif (self.config.sent_attention == 'max'):\n",
    "            tokens_output = tokens_output + tf.expand_dims((mask_tokens-1)*999,2)\n",
    "            tokens_output = tf.reduce_max(tokens_output, 1)\n",
    "\n",
    "        #Sentence level RNN\n",
    "        sents_input = tf.reshape(tokens_output, [batch_l*max_doc_l, max_ans_l,2*self.config.dim_sem])\n",
    "        ans_l = tf.reshape(ans_l,[batch_l*max_doc_l])\n",
    "        mask_sents = tf.reshape(mask_sents,[batch_l*max_doc_l,-1])\n",
    "\n",
    "        sents_output, _ = dynamicBiRNN(sents_input, ans_l, n_hidden=self.params.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/ans')\n",
    "\n",
    "        sents_sem = tf.concat([sents_output[0][:,:,:self.config.dim_sem], sents_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        sents_str = tf.concat([sents_output[0][:,:,self.config.dim_sem:], sents_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        temp1 = tf.zeros([batch_l * max_doc_l, max_ans_l, 1], tf.float32)\n",
    "        temp2 = tf.zeros([batch_l * max_doc_l, 1, max_ans_l], tf.float32)\n",
    "\n",
    "        mask1 = tf.ones([batch_l * max_doc_l , max_ans_l, max_ans_l-1], tf.float32)\n",
    "        mask2 = tf.ones([batch_l * max_doc_l , max_ans_l-1, max_ans_l], tf.float32)\n",
    "        \n",
    "        mask1 = tf.concat([temp1,mask1],2)\n",
    "        mask2 = tf.concat([temp2,mask2],1)\n",
    "\n",
    "        str_scores_ = get_structure('ans', sents_str, max_ans_l, mask1,mask2)  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        sents_sem_root = tf.concat([tf.tile(embeddings_root_a, [batch_l*max_doc_l, 1, 1]), sents_sem], 1)\n",
    "        sents_output_ = tf.matmul(str_scores, sents_sem_root)\n",
    "        sents_output = LReLu(tf.tensordot(tf.concat([sents_sem, sents_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.doc_attention == 'sum'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)\n",
    "        elif (self.config.doc_attention == 'mean'):\n",
    "            sents_output = sents_output * tf.expand_dims(mask_sents,2)\n",
    "            sents_output = tf.reduce_sum(sents_output, 1)/tf.expand_dims(tf.cast(ans_l,tf.float32),1)\n",
    "        elif (self.config.doc_attention == 'max'):\n",
    "            sents_output = sents_output + tf.expand_dims((mask_sents-1)*999,2)\n",
    "            sents_output = tf.reduce_max(sents_output, 1)\n",
    "\n",
    "        #Answer level RNN\n",
    "        ans_input = tf.reshape(sents_output, [batch_l, max_doc_l,2*self.config.dim_sem])\n",
    "        ans_output, _ = dynamicBiRNN(ans_input, doc_l, n_hidden=self.params.dim_hidden, cell_type=self.config.rnn_cell, cell_name='Model/doc')\n",
    "\n",
    "        ans_sem = tf.concat([ans_output[0][:,:,:self.config.dim_sem], ans_output[1][:,:,:self.config.dim_sem]], 2)\n",
    "        ans_str = tf.concat([ans_output[0][:,:,self.config.dim_sem:], ans_output[1][:,:,self.config.dim_sem:]], 2)\n",
    "\n",
    "        str_scores_ = get_structure('doc', ans_str, max_doc_l, self.t_variables['mask_parser_1'], self.t_variables['mask_parser_2'])  #batch_l,  sent_l+1, sent_l\n",
    "        str_scores = tf.matrix_transpose(str_scores_)  # soft parent\n",
    "        ans_sem_root = tf.concat([tf.tile(embeddings_root, [batch_l, 1, 1]), ans_sem], 1)\n",
    "        ans_output_ = tf.matmul(str_scores, ans_sem_root)\n",
    "        ans_output = LReLu(tf.tensordot(tf.concat([ans_sem, ans_output_], 2), w_comb, [[2], [0]]) + b_comb)\n",
    "\n",
    "        if (self.config.ans_attention == 'sum'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            # ans_output = tf.reduce_sum(ans_output, 1)\n",
    "        elif (self.config.ans_attention == 'mean'):\n",
    "            ans_output = ans_output * tf.expand_dims(mask_answers,2)\n",
    "            ans_output = tf.reduce_sum(ans_output, 1)/tf.expand_dims(tf.cast(doc_l,tf.float32),1)\n",
    "        elif (self.config.ans_attention == 'max'):\n",
    "            ans_output = ans_output + tf.expand_dims((mask_answers-1)*999,2)\n",
    "            ans_output = tf.reduce_max(ans_output, 1)\n",
    "\n",
    "        encoder_output = ans_output\n",
    "        tgt_vocab_size = self.params.vsize\n",
    "        learning_rate = self.config.lr\n",
    "        \n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(self.config.dim_output)\n",
    "        lstm_init = tf.contrib.rnn.LSTMStateTuple(encoder_output,encoder_output)\n",
    "        projection_layer = tf.layers.Dense(tgt_vocab_size, use_bias=False)\n",
    "\n",
    "#         Attention Decoder Call start\n",
    "        decoder_outputs,_dec_out_state,attn_dists,p_gens=attention_decoder(reference_input,lstm_init,token_encoder_states,decoder_cell)\n",
    "#         Attention Decoder Call end\n",
    "        \n",
    "        #training\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(reference_input, abstract_l, time_major=False)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, training_helper,initial_state=lstm_init,output_layer=projection_layer)\n",
    "        outputs, states, seq_l = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        training_logits = outputs.rnn_output\n",
    "        \n",
    "        #inference\n",
    "        embeddings = np.float32(self.params.embeddings)\n",
    "        start_tokens = tf.tile(tf.constant([self.params.inv_vocab['<GO>']], dtype=tf.int32), [batch_l], name='start_tokens')\n",
    "        \n",
    "        if(config.inference_mode == 'greedy'):\n",
    "            inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,start_tokens,self.params.inv_vocab['<EOS>'])\n",
    "            inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, inference_helper, lstm_init,output_layer=projection_layer)  \n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, maximum_iterations=self.config.max_summary_length)\n",
    "            inference_logits = outputs.sample_id\n",
    "\n",
    "        elif(config.inference_mode == 'beam'):\n",
    "            beam_decoder_initial_state = tf.contrib.seq2seq.tile_batch(lstm_init, multiplier=self.config.beam_width)\n",
    "            inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,embedding=embeddings,start_tokens=start_tokens,end_token=self.params.inv_vocab['<EOS>'],\n",
    "                                                                 initial_state=beam_decoder_initial_state,beam_width=self.config.beam_width,output_layer=projection_layer,\n",
    "                                                                 length_penalty_weight=0.0)\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,impute_finished = False, maximum_iterations=self.config.max_summary_length)\n",
    "            inference_logits = outputs.predicted_ids\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.t_variables['abstract_idxs'], logits=training_logits)\n",
    "        target_weights = tf.sequence_mask(abstract_l, max_abstract_l, dtype=tf.float32)\n",
    "        reduced_loss = tf.reduce_sum(loss*target_weights)/tf.to_float(batch_l)\n",
    "        \n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,epsilon=0.1)\n",
    "        update_step = optimizer.apply_gradients(zip(clipped_gradients, params),global_step=global_step)\n",
    "\n",
    "        self.final_output = training_logits\n",
    "        self.inference_logits = inference_logits\n",
    "        self.loss = reduced_loss\n",
    "        self.opt = optimizer.minimize(loss)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded succesfully\n"
     ]
    }
   ],
   "source": [
    "#Main function begins here\n",
    "config = flags.FLAGS\n",
    "\n",
    "remaining_args = flags.FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
    "assert(remaining_args == [sys.argv[0]])\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu)\n",
    "\n",
    "hash = random.getrandbits(32)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ah = logging.FileHandler(str(hash)+'.log')\n",
    "ah.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "ah.setFormatter(formatter)\n",
    "logger.addHandler(ah)\n",
    "\n",
    "gc.disable()\n",
    "train, dev, test, embeddings, vocab = pickle.load(open(config.data_file,'rb'))\n",
    "gc.enable()\n",
    "print('Data loaded succesfully')\n",
    "\n",
    "trainset, devset, testset = DataSet(train), DataSet(dev), DataSet(test)\n",
    "vocab = dict([(v.index,k) for k,v in vocab.items()])\n",
    "\n",
    "trainset.sort()\n",
    "devset.sort()\n",
    "testset.sort()\n",
    "\n",
    "train_batches = trainset.get_batches(config.batch_size, config.epochs, rand=True)\n",
    "dev_batches = devset.get_batches(config.batch_size, 1, rand=False)\n",
    "test_batches = testset.get_batches(config.batch_size, 1, rand=False)\n",
    "dev_batches = [i for i in dev_batches]\n",
    "test_batches = [i for i in test_batches]\n",
    "\n",
    "num_examples, train_batches, dev_batches, test_batches, embedding_matrix, vocab = len(train), train_batches, dev_batches, test_batches, embeddings, vocab\n",
    "\n",
    "n_embed,d_embed = embedding_matrix.shape\n",
    "vsize = len(vocab)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "dim_hidden = config.dim_sem+config.dim_str\n",
    "params = Params(n_embed,d_embed,vocab,inv_vocab,vsize,dim_hidden,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StructureModel(config,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_scope' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2f52417d930d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0e448ff73d26>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;31m#         Attention Decoder Call start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mattention_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;31m#         Attention Decoder Call end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a632b4a84c2a>\u001b[0m in \u001b[0;36mattention_decoder\u001b[0;34m(decoder_inputs, initial_state, encoder_states, cell)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattention_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_decoder\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mattn_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variable_scope' is not defined"
     ]
    }
   ],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "#Change number here (MILU)\n",
    "num = 36000\n",
    "\n",
    "    \n",
    "#training mode\n",
    "if(config.mode == 'train'):\n",
    "    num_batches_per_epoch = int(num_examples / config.batch_size)\n",
    "    num_steps = config.epochs * num_batches_per_epoch\n",
    "\n",
    "    with tf.Session(config=tfconfig) as sess:\n",
    "        gvi = tf.global_variables_initializer()\n",
    "        sess.run(gvi)\n",
    "        sess.run(model.embeddings.assign(embedding_matrix.astype(np.float32)))\n",
    "\n",
    "        model_name = 'Checkpoints/5/model'+str(num)+'.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ct, batch in tqdm.tqdm(train_batches, total=num_steps):\n",
    "            feed_dict = model.get_feed_dict(batch)\n",
    "            outputs,_,_loss = sess.run([model.final_output, model.opt, model.loss], feed_dict=feed_dict)\n",
    "            loss+=_loss\n",
    "            if(ct%config.log_period==0):\n",
    "                print ('Loss in',ct,' is: ',loss/config.log_period)\n",
    "                model_name = 'Checkpoints/5/model'+str(num+ct)+'.ckpt'\n",
    "                save_path = saver.save(sess, model_name)\n",
    "                loss = 0\n",
    "                \n",
    "elif(config.mode=='infer'):\n",
    "    #infer mode\n",
    "    with tf.Session(config=tfconfig) as sess:\n",
    "        gvi = tf.global_variables_initializer()\n",
    "        sess.run(gvi)\n",
    "        sess.run(model.embeddings.assign(embedding_matrix.astype(np.float32)))\n",
    "\n",
    "        model_name = 'Checkpoints/5/model'+str(num)+'.ckpt'\n",
    "        saver.restore(sess, model_name)\n",
    "        loss_ = 0\n",
    "        for ct, batch in tqdm.tqdm(test_batches,total=100):\n",
    "            feed_dict = model.get_feed_dict(batch)\n",
    "            answer_logits,loss = sess.run([model.inference_logits,model.loss], feed_dict=feed_dict)\n",
    "            loss = sess.run(model.loss,feed_dict=feed_dict)\n",
    "#             print(answer_logits)\n",
    "            loss_ +=loss\n",
    "            if(ct%config.log_period==0):\n",
    "                print(loss_/config.log_period)\n",
    "                loss_ = 0\n",
    "\n",
    "#             print('  Summary: {}'.format(\" \".join([vocab[j] for i in answer_logits for j in i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
